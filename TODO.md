* Understand dependence on alternatives (e.g. the credit card in Clark, or the other animals in our experiment). This seems tricky in general.
* Understand the clark model better
  * What parameters are most important
  * how do they affect predictions?
  * why can’t we get above 50% likelihood of ‘yes’/‘no’ answers?
  * let people say both ‘yes’/‘no’ and ‘information’, like Clark?
* Run experiment with bigger space
  * 6 nodes, arranged with three subtrees
  * Subsample sets for questions, answers, quds, etc, to get a wider space of predictions
  * This will hopefully reduce the ‘game show’ effect — the process of elimination that was suggested by the exact correspondence b/w questions and answers
* Run experiments with multiple players simultaneously, where they actually get an answer and actually pick a gate
  * This should give us a better intuition about learning & audience design (do questioners adjust to answerer behavior?) 
  * should make the communication context more natural since there’s some time pressure, and we can allow for some degree of free response? 
  * Turn it into a taboo-type game, maybe?
* Run Bayesian analysis to estimate explicit/pragmatic mixture, and how we can push this around
* Figure out the correspondence b/w RSA and van Rooy decision-theory approach…
* Figure out what other utilities we can model in the same (e.g. casual conversation? Polite requests? Pedagogical questions to test answerer knowledge?)
* Check that the model specification of literal and explicit answerer in the paper matches our code, and try to clarify. 
  * Our current notation P_q(w|q, a) is a bit difficult to read. 





