// Run as:
// webppl genAnswererPredictions.wppl --require ../qa

var main = function(){

  // setting ranges
  var rationalityPs = _.range(1, 10, .1);
  var types = ["branching", "overlapping", "equivocal"];
  var domains = ["artifact", "animals", "places", "plants"];
  var uniformSettings = [true, false];

  map(function(assumeUniform) {
    // different files for different priors
    var uniformStr = assumeUniform ? "Unif" : "Emp";
    var fileName = "answererPredictions" + uniformStr + ".raw.csv";
    qa.writeCSV([["type", "domain", "modelLevel",
		  "utterance", "rationality", "response", "modelProb"]], fileName);
    map(function(type) {
      map(function(domain) {
	map(function(rationality) {
	  // Set up file
	  var beta = assumeUniform ? 0 : 1;
	  var model = QAmodel(type, domain, rationality, beta);
	  var runAnswererModel = model.runAnswererModel;

	  // Set up r to be nice
	  var r = rationality.toFixed(1);

	  map(function(question) {
            // // get relevant parameters
	    var shortQuestion = qa.butLast(question.split("Is")[1].toLowerCase());

            var litA = model.litA;
            var litERP = runAnswererModel(litA, question);
            var litLabel = [type, domain, "literal", shortQuestion, r];
	    qa.writeERP(litERP, litLabel, fileName, 2);

            var pragA = model.pragA;

	    var pragERP = runAnswererModel(pragA, question);
            var pragLabel = [type, domain, "pragmatic", shortQuestion, r];
	    qa.writeERP(pragERP, pragLabel, fileName);

            var expA = model.expA;		       
	    var expERP = runAnswererModel(expA, question);
	    var expLabel = [type, domain, "explicit", shortQuestion, r];
	    qa.writeERP(expERP, expLabel, fileName);	    
	  }, model.questionSpace);
	}, rationalityPs);
      }, domains);
    }, types);
  }, uniformSettings);
  return 'done';
};

main();
