---
title: "Questions and Answers in Dialogue"
output: html_notebook
---

# Imports 

```{r}
library(tidyverse)

library(ggthemes)
library(GGally)
library(coda)
library(boot)
library(cowplot)

source("./analysisHelpers.R")

options("scipen"=1) 
```

Experiment data analysis
--------------------------

Start by setting up exclusion criteria for participants who don't speak english or didn't complete the full task.

```{r}
subjInfo = read_csv("../data/compiled-subject_information.csv")

mturk = read_csv("../data/compiled-mturk.csv") %>%
  mutate(gameid = Answer.id) %>%
  select(workerid, gameid)

rawAggregated = read_csv("../data/compiled-trials_clean.csv") %>%
  right_join(mturk, by = 'workerid') %>%
  right_join(subjInfo, by = 'workerid')
```

Filter out participants who didn't complete all 12 trials, had missing data (i.e. didn't answer for some subset of trials), or didn't list english as their native language, then keep one of each game

```{r}
nonNativeSpeaker <- rawAggregated %>% 
  filter(nativeEnglish != "yes")

incomplete = rawAggregated %>% 
  group_by(gameid) %>% 
  count(gameid) %>% 
  mutate(numCompleted = n) %>% 
  filter(numCompleted != 24)

missingData = rawAggregated %>%
  filter(is.na(answer))

badGames <- union(
  nonNativeSpeaker$gameid, 
  union(incomplete$gameid, missingData$gameid)
)

d <- rawAggregated %>%
  filter(!(gameid %in% badGames)) %>%
  distinct(domain, goal, question, guess, answer, type, gameid)
  
write_csv(d, "../data/BayesianAnalysisInput.csv")
```

In order to compare different items in a convenient way, we're going to map the questions and answers to the corresponding node positions in the hierarchy. So, in 'branching' trials, for example, 'dalmatian', 'mansion', 'carrot', and 'couch' would all be treated the same.


```{r}
d <- mapWordsToNodes(d)
```

We're also going to estimate empirical probabilities for each response, conditioned on the domain, type, and goal of the trial. To get confidence intervals for these estimates, we'll use the bootstrap. Tidy up questioner data...

```{r}
d_q = d %>% 
      mutate(response=ordered(questionNodes, levels=c("Q1","Q2","Q3","Q4"))) %>%
      mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
      group_by(domain, type, goal) %>%
      do(getProbsAndCIs(data = ., QorA = 'q', R = 1000, FALSE)) %>%
      select(goal, type, response, domain, count, lower_ci, upper_ci,
             groupSize, empProb) %>%
      ungroup() %>%
      # Note that there are no Q3 & Q4 in the equivocal condition
      filter(!(type == "equivocal" & (response == "Q3" | response == "Q4"))) %>%
      mutate(domain = factor(domain),
             type = factor(type),
             goal = ordered(goal, levels=c("G1","G2","G3","G4")))
```

Tidy up answerer data...

```{r}
d_a = d %>% 
      mutate(response=ordered(answerNodes,levels=c("A1","A2","A3","A4"))) %>%
      mutate(utterance=ordered(questionNodes,levels=c("Q1","Q2","Q3","Q4"))) %>%
      group_by(domain, type, utterance) %>%
      do(getProbsAndCIs(data = ., QorA = 'a', R = 1000, FALSE)) %>%
      select(utterance, type, response, domain, count,lower_ci, upper_ci,
             groupSize, empProb) %>%
      ungroup() %>%
      mutate(domain = factor(domain),
             type = factor(type),
             utterance = ordered(utterance, levels=c("Q1","Q2","Q3","Q4")))
```

# Qualitative Behavioral Results

### Correlations between domains

```{r}
col1 = subset(d_q, domain == "animals")$empProb
col2 = subset(d_q, domain == "places")$empProb
col3 = subset(d_q, domain == "plants")$empProb
col4 = subset(d_q, domain == "artifact")$empProb
corData_q = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cat('questioner domain correlations:\n')
cor(corData_q)

col1 = subset(d_a, domain == "animals")$empProb
col2 = subset(d_a, domain == "places")$empProb
col3 = subset(d_a, domain == "plants")$empProb
col4 = subset(d_a, domain == "artifact")$empProb
corData_a = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cat('\nanswerer domain correlations:\n')
cor(corData_a)
```

### Answerer deviation from uniform

for example:

```{r}
# Collapse across domains
overall_distribution = d_a %>% 
  group_by(utterance, type, response) %>% 
  summarize(count = sum(count))

overall_distribution %>% 
  filter(utterance == 'Q4' & type == 'branching') %>%
  mutate(prob = count / sum(count))
```

get CIs for plots

```{r}
collapsed_a = d %>% 
      mutate(response=ordered(answerNodes,levels=c("A1","A2","A3","A4"))) %>%
      mutate(utterance=ordered(questionNodes,levels=c("Q1","Q2","Q3","Q4"))) %>%
      group_by(type, utterance) %>%
      do(getProbsAndCIs(data = ., QorA = 'a', R = 1000, TRUE)) %>%
  mutate(source='empirical')
```

get collapsed model predictives for plot

```{r}
A1 = getRawPredictives('answerer_modelComparisonPredictives', chosenModelType = 'A1_unif') %>%
  filter(posteriorProb == max(posteriorProb)) %>%
  mutate(source = 'A1') 
A2 = getRawPredictives('answerer_modelComparisonPredictives', chosenModelType = 'A2_unif') %>%
  filter(posteriorProb == max(posteriorProb)) %>%
  mutate(source = 'A2')

models = rbind(A1, A2) %>%
  select(parameter, item1, item2, value, prediction, source) %>%
  rename(question = parameter, answer = value, type = item1, domain = item2) %>%
  do(mutate(., answer = vectorizedMapAnswer(type, gsub(".$", "", answer)))) %>%
  do(mutate(., question = vectorizedMapQuestion(type, question))) %>%
  filter(type == 'branching' & question == "Q4") %>%
  select(domain, answer, prediction, source) %>%
  group_by(answer, source) %>%
  summarize(prob = mean(prediction)) %>%
  ungroup() %>%
  mutate(lower_ci = NA, upper_ci = NA) 
```

Plot:

```{r}
cbPalette = c(rgb(0, .6, .4), rgb(0, .4, .6), rgb(.4, .4, .4))
collapsed_a %>% 
  filter(utterance == "Q4" & type == "branching") %>%
  ungroup() %>%
  select(-type, -count, -utterance, -groupSize) %>%
  rename(prob = empProb, answer = response) %>%
  mutate(source = 'empirical') %>%
  rbind(models) %>%
  mutate(answer = factor(answer, levels = c('A1', 'A2', 'A3', 'A4'), 
                       labels = c('dalmatian', 'poodle', 'cat', 'whale'))) %>%
  ggplot(aes(x = answer, y = prob, fill = source)) +
    geom_bar(stat='identity', position = 'dodge') +
    geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), color = 'gray', 
                  position = position_dodge(width = 0.9), width = .25) +
    theme_few(10) +
    scale_fill_manual(values=cbPalette) +
    facet_wrap(~ source) +
    ylim(0,1) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    ggtitle("answer in branching condition\nwhen asked 'Where is the animal?'")

ggsave("../writing/2016/journal-manuscript/figures/BranchingModelComparison.pdf", 
       width = 4, height=3)
```

statistical test:

```{r}
chisq.test(subset(overall_distribution, 
                  utterance == "Q4"& type == "branching")$count)
```

### Questioner overlapping condition

Make plot

```{r}
gs = c("G1", "G2", "G3", "G4")
qs = c("Q1","Q2","Q3","Q4")
collapsed_q <- d %>% 
  mutate(response = ordered(questionNodes, levels = qs)) %>%
  mutate(goal = ordered(goalNodes, levels = gs)) %>%
  group_by(goal,type) %>%
  filter(type == "overlapping") %>%
  do(getProbsAndCIs(data = ., QorA = 'q', R = 1000, TRUE)) %>%
  mutate(empirical = empProb) %>%
  select(goal, type, response, count, lower_ci, upper_ci, 
         groupSize, empirical) %>%
  mutate(source = 'empirical')
```

```{r}
Q1 = getRawPredictives('questioner_modelComparisonPredictives', chosenModelType = 'Q1_unif') %>%
  filter(posteriorProb == max(posteriorProb)) %>%
  mutate(source = 'Q1') 
Q2 = getRawPredictives('questioner_modelComparisonPredictives', chosenModelType = 'Q2_unif') %>%
  filter(posteriorProb == max(posteriorProb)) %>%
  mutate(source = 'Q2')

models = rbind(Q1, Q2) %>%
  select(parameter, item1, item2, value, prediction, source) %>%
  rename(goal = parameter, question = value, type = item1, domain = item2) %>%
  do(mutate(., goal = vectorizedMapGoal(type, goal))) %>%
  do(mutate(., question = vectorizedMapQuestion(type, question))) %>%
  mutate(goal = ordered(goal, levels = gs)) %>%
  filter(type == 'overlapping' & goal == "G2") %>%
  select(domain, question, prediction, source) %>%
  group_by(question, source) %>%
  summarize(prob = mean(prediction)) %>%
  ungroup() %>%
  mutate(lower_ci = NA, upper_ci = NA) 
```

plot 

```{r}
cbPalette = c(rgb(0, .6, .4), rgb(0, .4, .6), rgb(.4, .4, .4))
plottableQ = collapsed_q %>% 
  filter(goal == "G2") %>%
  ungroup() %>%
  select(-type, -count, -goal, -groupSize) %>%
  rename(prob = empirical, question = response) %>%
  mutate(source = 'empirical') %>%
  rbind(models) %>%
  mutate(question = factor(question, levels = qs, 
                           labels = c('lion?', 'cat?', 'pet?', 'animal?'))) %>%
  mutate(source = ordered(source, levels = c('Q1', 'Q2', 'empirical')))

(ggplot(plottableQ, aes(x = question, y = prob, fill =source))
 + geom_bar(position = 'dodge', stat= 'identity')
 + geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), 
                 position =position_dodge(width = 0.9), width =.25, color = 'gray') 
 + ggtitle("question in overlapping condition\nwhen goal is to find 'house cat'")
 + theme_few(10)
 + scale_fill_manual(values=cbPalette) 
 + facet_wrap(~ source)
 + ylim(0,1)
 + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
 + ylab('% participants'))


ggsave("../writing/2016/journal-manuscript/figures/OverlappingModelComparison.pdf", 
       width = 4, height=3)
```

get statistical result

```{r}
cat('Q2:', round(subset(plottableQ, response == 'cat?')$empirical, 2))
cat('\nQ3:', round(subset(plottableQ, response == 'pet?')$empirical, 2))

diffScore <- function(data, indices) {
  d <- data[indices,] %>%
    group_by(response) %>%
    summarize(count = n()) %>%
    ungroup() %>%
    mutate(prob = count / sum(count)) %>%
    select(-count) %>%
    spread(response, prob)
  return(d$Q2 - d$Q3)
}

bootRes = d %>% 
  mutate(response = ordered(questionNodes, levels = c("Q1","Q2","Q3","Q4"))) %>%
  mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
  group_by(goal,type) %>%
  filter(type == "overlapping") %>%
  filter(goal == "G2") %>%
  boot(statistic = diffScore,R=1000) 

estimate <- round(bootRes$t0,2)
lowerDiffScore <- round(boot.ci(bootRes, type = "perc")$percent[4],2)
upperDiffScore <- round(boot.ci(bootRes, type = "perc")$percent[5],2)
cat(paste0(c('\nQ3-Q2 = ', estimate,
             '[', lowerDiffScore, 
             ', ', upperDiffScore, ']')))
```

# Interactive Experiment

## Model Comparison 

```{r}
modelComparison <- function(name, useTypicality) {
  # beta <- ifelse(useTypicality, NA, 0)
  data <- getRawParams(name)#, chosenBeta = beta)
  return(data %>% 
    group_by(modelType) %>% 
    summarize(prob = reduce(logLikelihood, sumlogprob) - log(length(logLikelihood)))
  )
}
```

Answerers:

```{r}
# Marginalize over alpha
comp.ans = modelComparison("answerer_modelComparisonParams")%>% 
  spread(modelType, prob) 
cat('\nBF (beta=0, answerer)', exp(comp.ans$A2_unif - comp.ans$A1_unif))

modelComparison("answerer_modelComparisonParams")%>% 
  arrange(prob) %>%
  separate(col = modelType, into = c('model', 'saliency')) %>%
  mutate(saliency = ordered(saliency, levels = c('emp', 'unif'), labels = c('empirical', 'uniform'))) %>%
  mutate(model = reorder(model, order(prob, decreasing = T))) %>%
  ggplot(aes(x = model, y = prob, fill = saliency)) +
    geom_bar(stat = 'identity', position = 'dodge') +
    guides(fill=guide_legend(title="Saliency prior")) +
    ylab('log-likelihood') +
    scale_fill_colorblind() +
    theme_few(12) +
    theme(legend.position = 'top') 


ggsave("../writing/2016/journal-manuscript/figures/answererLogLikelihoods.pdf", 
       width = 3.5, height=3)
```

```{r}
comp.quest = modelComparison("questioner_modelComparisonParams") %>% 
  spread(modelType, prob) 
cat('\nBF (beta=0, questioner)', exp(comp.quest$Q2_unif - comp.quest$Q1_unif))

modelComparison("questioner_modelComparisonParams")%>% 
  arrange(prob) %>%
  separate(col = modelType, into = c('model', 'saliency')) %>%
  mutate(saliency = ordered(saliency, levels = c('emp', 'unif'), labels = c('empirical', 'uniform'))) %>%
  mutate(model = ordered(model, levels = c('Q2', 'Q1', 'Q0'), labels = c('Q2', 'Q1', 'Q0'))) %>%
  ggplot(aes(x = model, y = prob, fill = saliency)) +
    geom_bar(stat = 'identity', position = 'dodge') +
    guides(fill=guide_legend(title="Saliency prior")) +
    ylab('log-likelihood') +
    scale_fill_colorblind() +
    theme_few(12) +
    theme(legend.position = 'top')

ggsave("../writing/2016/journal-manuscript/figures/questionerLogLikelihoods.pdf", 
       width = 3.5, height=3)
```

Inset

```{r}
modelComparison("questioner_modelComparisonParams")%>% 
  arrange(prob) %>%
  separate(col = modelType, into = c('model', 'saliency')) %>%
  mutate(saliency = ordered(saliency, levels = c('emp', 'unif'), labels = c('empirical', 'uniform'))) %>%
  mutate(model = ordered(model, levels = c('Q2', 'Q1', 'Q0'), labels = c('Q2', 'Q1', 'Q0'))) %>%
  filter(model != "Q0") %>%
  ggplot(aes(x = model, y = prob, fill = saliency)) +
    geom_bar(stat = 'identity', position = 'dodge') +
    guides(fill=guide_legend(title="Saliency prior")) +
    ylab('log-likelihood') +
    scale_fill_colorblind() +
    theme_few(12) +
    coord_cartesian(ylim=c(-360, -275)) +
    theme(legend.position = 'top')

ggsave("../writing/2016/journal-manuscript/figures/questionerLogLikelihoodsZoom.pdf", 
       width = 3.5, height=3)
```

## Examine parameter posteriors

Set up helper functions. Note that we only look at the parameter posterior for the model that won our model comparison (otherwise this would be a mixture of the best parameters for each model we considered)

```{r}
examineParams <- function(name, useTypicality, winningModel) {
  beta <- ifelse(useTypicality, NA, 0)

  samples = getRawParams(name, chosenBeta = beta, chosenModelType = winningModel) %>% 
    mutate(MCMCprob = exp(MCMCprob)) %>%
    filter(MCMCprob > 0.001) %>%
    mutate(n = floor(MCMCprob*1000)) %>%
    do(data.frame(.[rep(1:nrow(.), .$n),])) %>%
    select(-t, -n, -MCMCprob) %>%
    gather(parameter, value) %>%
    mutate(value = as.numeric(as.character( value)))

  printTable(samples)
  
  questionParamPosterior <- ggplot(samples, aes(x = value))+
      geom_histogram(aes(y=..density..), 
                   data =subset(samples, parameter == "alpha" ), 
                   binwidth = .25, colour="black", fill="white") +
      geom_histogram(aes(y=..density..), 
                   data=subset(samples, parameter == "beta"), 
                   binwidth = .05, colour="black", fill="white") +
      geom_density(aes(y=..density..),
                   data =subset(samples, parameter == "alpha" ), 
                   adjust = 2, alpha=.2, fill="#FF6666")+
      geom_density(aes(y=..density..),
                   data=subset(samples, parameter == "beta"), 
                   adjust = 5, alpha=.2, fill="#FF6666")+
      facet_wrap(~parameter, scales = 'free') +
      theme_few(9)
  
  print(questionParamPosterior)
  outputName = paste0("../writing/2016/journal-manuscript/figures/",
                     name, ".pdf")
  ggsave(outputName, questionParamPosterior, 
         width = 6, height = 3, bg = "transparent")
}
```

Answerer (no typicality)

```{r}
examineParams('answerer_modelComparisonParams', useTypicality=FALSE, winningModel = 'pragmatic')
```

Questioner (no typicality)

```{r}
examineParams('questioner_modelComparisonParams', useTypicality=TRUE, winningModel = 'pragmatic')
```

## Examine posterior predictives

```{r}
examinePredictives <- function(name, winningModel) {
  inputname <- paste0('../../guessingGame/Bayesian/data/', name, '.csv')
  beta <- NA
  QorA <- toupper(substr(strsplit(name, '_')[[1]][1], 0, 1))
  samples = getRawPredictives(name, chosenBeta = beta, chosenModelType = winningModel) %>% 
    rawPredictivesToSamples()

  predictive = recode(samples, QorA)

  cor_label = round(cor(predictive$MAP, predictive$empProb), 2)
  title = paste0(winningModel, ' ', strsplit(name, '_')[[1]][1], " Posterior Predictive")
  
  predictive.plot = (ggplot(predictive, aes(x = MAP, y = empProb))
    + xlab("")
    + ylim(0,1)
    + ylab("")
    #+ geom_errorbar(aes(ymax = upper_ci, ymin = lower_ci))
    #+ geom_errorbarh(aes(xmax = credHigh, xmin = credLow))
    + geom_point(alpha = .5, size=2)#aes(colour = domain, shape = type),  size = 2)
    + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
    + scale_x_continuous(lim = c(0,1), breaks=c(0,.5,1))
#    + ggtitle(title)
    #+ geom_smooth(method = "lm")
    + annotate('text', label = paste0('r =',cor_label), x= 0.8, y = 0.1)
    + coord_fixed()
    + theme_few(12)
    + theme(aspect.ratio = 1) 
    # + theme(rect = element_rect(fill = "transparent"), # bg of the panel
    #         panel.background = element_rect(fill = "transparent",colour = NA),
    #         plot.background = element_rect(fill = "transparent",colour = NA),
    #         panel.grid.major = element_blank(), # get rid of major grid
    #         panel.grid.minor = element_blank(), # get rid of minor grid
    #         legend.background = element_rect(fill = "transparent",colour = NA),
    #         legend.box.background = element_rect(fill = "transparent",colour = NA) # get rid of legend panel bg
  )
  if(winningModel %in% c('Q2_emp', 'Q1_emp')) {
    predictive.plot <- predictive.plot + 
      geom_point(data = subset(predictive, type == 'overlapping'), 
                 aes(x = MAP, y = empProb), color = 'coral')
  }
  outputfile <- paste0('../writing/2016/journal-manuscript/figures/',
                       name, '_model', winningModel, '.pdf')
  ggsave(outputfile, predictive.plot,
         width = 3, height = 3, bg = "transparent")
  return(predictive.plot)
}
```

Answerer (no typicality)

```{r}
A1 = examinePredictives('answerer_modelComparisonPredictives', winningModel = 'A1_unif')
A2 = examinePredictives('answerer_modelComparisonPredictives', winningModel = 'A2_unif')
#plot_grid(A1, A2, A3, A4, nrow = 2, ncol = 2)
```

Questioner (no typicality)

```{r}
Q1 = examinePredictives('questioner_modelComparisonPredictives', winningModel = 'Q1_unif')
Q2 = examinePredictives('questioner_modelComparisonPredictives', winningModel = 'Q2_unif')
#plot_grid(Q1, Q2, nrow = 1)
```

# Enriching with typicality

```{r}
typPs = read.csv("../data/priorsExperiment-subject_information.csv")

nonNative = typPs %>% 
  filter(!(language %in% c("english", "English")))

confused = typPs %>% 
  filter(asses != "Yes")

badGames = union(nonNative$workerid, confused$workerid)

cat("We removed", length(nonNative$workerid), "ps due to native language")
cat("and", length(setdiff(confused$workerid, nonNative$workerid)), "more due to confusion")

typData = read.csv("../data/priorsExperiment-trials.csv") %>%
  filter(!(workerid %in% badGames)) 

totalCounts = typData %>%
  group_by(domain,type,label) %>%
  summarize(total = n()) 

proportions = typData %>%
  inner_join(totalCounts) %>%
  group_by(domain,type,label,response) %>%
  summarize(prop = n() / mean(total))
```

## Deviations from uniformity

```{r}
chisq.pvals = typData %>% 
  count(domain, type, label, response) %>%
  group_by(domain, type, label) %>%
  filter(length(n) > 1) %>%
  do(data.frame(stat = chisq.test(.$n)$p.value)) %>%
  ungroup() %>%
  select(stat)
length(which(chisq.pvals < 0.05/length(chisq.pvals$stat))) / length(chisq.pvals$stat)
```

### Model comparison

```{r}
# Marginalize over alpha
comp.ans = modelComparison("answerer_modelComparisonParams") %>% 
  spread(modelType, prob) 
cat('\nBF (beta=0, questioner)', exp(comp.ans$A2_emp - comp.ans$A1_emp))

comp.quest = modelComparison("questioner_modelComparisonParams") %>% 
  spread(modelType, prob) 
cat('\nBF (beta=0, questioner)', exp(comp.quest$Q2_emp - comp.quest$Q1_emp))
```

### Typicality test

Savage-Dickey

```{r}
exp(comp.ans$A2_emp - comp.ans$A2_unif)
exp(comp.quest$Q2_emp - comp.quest$Q2_unif)
```

### Examine predictives 

Answerer 

```{r}
A1 = examinePredictives('answerer_modelComparisonPredictives', winningModel = 'A1_emp')
A2 = examinePredictives('answerer_modelComparisonPredictives', winningModel = 'A2_emp')
#plot_grid(A1, A2)
```

Questioner

```{r}
Q1 = examinePredictives('questioner_modelComparisonPredictives', winningModel = 'Q1_emp')
Q2 = examinePredictives('questioner_modelComparisonPredictives', winningModel = 'Q2_emp')
#plot_grid(Q1, Q2)
```
