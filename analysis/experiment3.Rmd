---
title: "R Notebook"
output: html_notebook
---


```{r}
library(tidyverse)
library(ggthemes)
library(tidyboot)
```

```{r}
d.subj.raw <- read_csv('../data/experiment3/exitSurveyFromMongo.csv') %>%
  filter(iterationName == 'full_sample')
d.goals.raw <- read_csv('../data/experiment3/goalInferenceFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.questions.raw <- read_csv('../data/experiment3/questionFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.answers.raw <- read_csv('../data/experiment3/answerFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 

# nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- union(unique((d.goals.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       union(unique((d.questions.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       unique((d.answers.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid)))
confused <- unique((d.subj.raw %>% filter(confused != 'yes'))$gameid)
nonNative <- unique((d.subj.raw %>% filter(nativeEnglish != 'yes'))$gameid)

badGames <- c(incompleteIDs, nonNative, confused)
d.answers <- left_join(
  d.answers.raw %>% filter(!(gameid %in% badGames)), 
  d.goals.raw %>% filter(!(gameid %in% badGames)) %>% arrange(gameid, trialNum),
  by = c('iterationName', 'gameid', 'trialNum', 'questionNumber', 'trialType', 'cellAskedAbout', 'firstRole', 'gridState')
) %>%
  select(-time.x, -time.y)
d.questions <- d.questions.raw %>% filter(!(gameid %in% badGames))
```

```{r}
cat(length(unique((d.questions.raw %>% group_by(gameid) %>% tally())$gameid)), 'initially recruited')
cat(length(incompleteIDs), 'were incomplete')
cat(length(unique((d.questions %>% group_by(gameid) %>% tally())$gameid)), 'left')
```

```{r}
write_csv(d.answers, '../data/experiment3/answerFromMongo_clean.csv')
write_csv(d.questions, '../data/experiment3/questionFromMongo_clean.csv')
```

## Goal inference 

Note that if you split out by trueGoal, there seems to be a ROW bias (probably bcecause ROW button was first...)
 
We also exclude 'random' style trials for this qualitative analysis since they were all over the place in what goal we predicted people would infer...   
 
```{r}
goal_response_proportions <- d.answers %>% 
  filter(!(trialType %in% c('practice', 'random'))) %>%
  mutate(correct = ifelse(goalResponse == 'not sure', '"not sure"', 
                          ifelse(trueGoal == goalResponse, 'correct', 'incorrect')),
         trialType = ifelse(trialType == 'empty', 'empty', 'pragmatic')) %>%
  filter(questionNumber < 4) 

goal_response_proportions.boot <- map_dfr(seq_len(5000), ~goal_response_proportions %>%
    group_by(trialType, questionNumber) %>%
    mutate(total = length(correct)) %>%
    do(sample_n(., nrow(.), replace=TRUE)) %>%
    group_by(correct, trialType, questionNumber) %>%
    summarize(prop = length(total)/mean(total)) %>%
    mutate(sample_num = .x)) %>%
    group_by(trialType, questionNumber, correct) %>%
    summarize(empirical_prop = mean(prop), ci_upper = ci_upper(prop), ci_lower = ci_lower(prop)) %>%
    ungroup() %>%
    mutate(correct = factor(correct, levels = c('correct', 'incorrect',  '"not sure"')),
          trialType = factor(trialType, levels = c('empty', 'pragmatic')),
          questionNumber = as.character(questionNumber)) %>%
    complete(questionNumber, trialType, correct, 
           fill = list(empirical_prop = 0.002, ci_upper = 0.003,ci_lower =0.001 )) #%>%

ggplot(goal_response_proportions.boot, aes(x = questionNumber, y = empirical_prop, color = correct, fill = correct, group=correct))  +
  #geom_bar(stat = 'identity', position = dodge) +
  geom_line() +
  theme_few() +
  facet_wrap(~ trialType) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2,color = NA) +#, width = 0, position = dodge) +
  scale_fill_colorblind() +
  scale_color_colorblind() +
  
  ylab('% responses') +
  #xlab('')+
  ylim(0, 1) +
  ggtitle("What goal do you think the leader has?")  +
  theme(aspect.ratio = 1)
  
ggsave('../writing/2019/journal-revision/figures/spatialGoalInference.pdf', width=14, height = 10, units='cm')
```

```{r}
goal_response_proportions.boot
goal_response_proportions %>% group_by(questionNumber, trialType, correct) %>% tally()
chisq.test(matrix(c(33, 40, 31, 9, 79, 16), nrow = 3))
```

# Qualitative model predictions

## Questioner

```{r}
qualitativeQ <- read_csv('../modeling/qa/questionFromMongo_fixed.csv')  %>%
  select(-sender, -timeFromMessage, -firstRole)

qualitativeQ %>%
  filter(trialType == 'blocked') %>%
  filter(questionNumber == 2) %>%
  group_by(initAskedAbout, secondAskedAbout) %>%
  tally()
```

```{r}
diffScore <- function(data, indices) {
  d <- data[indices,] %>%
    group_by(qualitativeQuestion) %>%
    summarize(count = n()) %>%
    ungroup() %>%
    mutate(prob = count / sum(count)) %>%
    select(-count) %>%
    spread(qualitativeQuestion, prob)
  return(d$pragmatic - d$confusing)
}

bootRes <- qualitativeQ %>% 
  filter(qualitativeTrialType == "ambiguous") %>%
  ungroup() %>%
  boot::boot(diffScore, R = 1000)

estimate <- round(bootRes$t0,2)
lowerDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[4],2)
upperDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[5],2)
cat(paste0(c('\npragmatic-confusing = ', estimate,
             '[', lowerDiffScore, 
             ', ', upperDiffScore, ']')))

```

## Answerer

See how often people *answer* with extra info.

```{r}
d.answers <- read_csv('../modeling/qa/answerFromMongo_fixed.csv') %>%
  group_by(gameid, trialNum) %>%
  mutate(trialType = first(trialType))

answer_info <- d.answers %>% 
  filter(questionNumber == 1) %>%
  filter(!(trialType %in% c('practice'))) %>%
  rowwise() %>%
  mutate(additionalInfo = ifelse(nchar(answer) == 11, 1, 0)) %>%
  mutate(trialType = ifelse(trialType == 'blocked', 'pragmatic', trialType)) %>%
  group_by(trialType) %>%
  tidyboot_mean(additionalInfo) %>% 
  ungroup() %>% 
  mutate(trialType = factor(trialType, levels = c('practice', 'pragmatic', 'empty')))
dodge = position_dodge(.9)
ggplot(answer_info, aes(x = trialType, y = empirical_stat))  +
  geom_bar(stat = 'identity', position = dodge) +
  theme_few() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
  scale_fill_few() +
  ylim(0,1) +
  ylab('% answerers give additional info') +
  xlab('')+
  ggtitle("") 
```

Look specifically at 'blocked' trials -- how do people respond?

```{r}
 d.answers %>% 
  filter(trialType == 'blocked') %>%
  filter(questionNumber == 1) %>%
    group_by(askedAboutStatus, answerType) %>%
    tally() %>%
    group_by(askedAboutStatus) %>%
    mutate(prop = n / sum(n))
```

# Quantitative model comparison

First, since it's pretty expensive to run the model for board in the dataset (without some symmetry collapsing tricks which we may want to do later anyway), we'll score the data under a particular rationality param setting. The relative likelihood of the explicit & pragmatic models shouldn't be affected that much by the rationality (just the absolute fits).

Begin by pruning huge csv of predictions

```{r}
uniqueWorldsInData <- unique(d.answers$underlyingWorld)
uniqueStatesInData <- unique(d.answers$gridState)

d.answer_predictions <- read_csv('../modeling/experiment3/spatialAnswererOutput_HierarchicalVersion.csv')

d.answer_predictions_trimmed <- d.answer_predictions %>% 
  rowwise() %>% 
  filter(world %in% uniqueWorldsInData) %>%
  filter(initState %in% uniqueStatesInData)
  
write_csv(d.answer_predictions_trimmed, '../modeling/experiment3/relevantAnswererOutput.csv')
```

```{r}
library(jsonlite)
Qout = fromJSON( '../modeling/experiment3/bdaOutput/dataAnalysisOut_questioner.json', flatten = T) %>%
  distinct()

Qout %>%
  group_by(questionerType) %>%
  filter(likelihood == max(likelihood)) %>%
  select(questionerType, likelihood) %>%
  spread(questionerType, likelihood) %>%
  mutate(maxLL = pragmatic - explicit)
```

```{r}
library(jsonlite)
answererModelComparison = fromJSON( '../modeling/experiment3/bdaOutput/dataAnalysisOut_answerer.json', flatten = T) %>%
  distinct()

answererModelComparison %>%
  group_by(answererType) %>%
  filter(likelihood == max(likelihood)) %>%
  select(answererType, likelihood) %>%
  distinct() %>%
  spread(answererType, likelihood) %>%
  mutate(maxLL = pragmatic - explicit)


answererModelComparison %>% 
  group_by(answererType) %>% 
  summarize(prob = reduce(likelihood, sumlogprob) - log(length(likelihood))) %>%
  spread(answererType, prob) %>%
  mutate(BF = pragmatic-explicit)
```

## Try to parse predictives 

```{r}
dodge = position_dodge(.9)
path = '../modeling/experiment3/bdaOutput/'
A.modelpredictions <- rbind(
    read_csv(paste0(path, 'answererPredictives_explicit.json')) %>% mutate(model = 'A1'),
    read_csv(paste0(path, 'answererPredictives_pragmatic.json')) %>% mutate(model = 'A2')
  ) %>%
  mutate(answer = gsub('\"', '', answer, fixed = T)) %>%
  mutate(answer = gsub('[', '', answer, fixed = T)) %>%
  mutate(answer = gsub(']', '', answer, fixed = T)) %>%
  filter(trialNum > 6) %>%
  mutate(questionNumber = paste0('question # ', questionNumber)) %>%
  mutate(modelScore = exp(modelScore)) 

A.binned <- A.modelpredictions %>%
  filter(model == 'A2') %>%
  mutate(correct = as.numeric(modelOption == answer)) %>%
  mutate(modelProbBin = .1*floor(modelScore / .1) + .05) %>%
  group_by(model, modelProbBin) %>%
  tidyboot_mean(correct)

ggplot(A.binned, aes(x = modelProbBin, y = mean)) +
    geom_point(aes(size = n), stat = 'identity') +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1) +
    geom_abline(intercept = 0, slope = 1) +
    #facet_wrap(~ model) +
    theme_bw() +
    xlim(0,1) +
    theme(aspect.ratio = 1) +
    ylab('human responses') +
    xlab('model predictions')

ggsave('../writing/2019/journal-revision/figures/Exp3/answererPredictives.pdf',  
       width=14, height = 10, units='cm', useDingbats=FALSE)

```

```{r}
Q.modelpredictions <- rbind(
    read_csv(paste0(path, 'questionerPredictives_explicit.json')) %>% mutate(model = 'Q1'),
    read_csv(paste0(path, 'questionerPredictives_pragmatic.json')) %>% mutate(model = 'Q2')
  ) %>%
  filter(trialNum > 6) %>%
  #filter(questionNumber <= 2) %>%
  mutate(questionNumber = paste0('question # ', questionNumber), 
         modelScore = exp(modelScore),
         correct = as.numeric(modelOption == question),
         modelProbBin = .1*floor(modelScore / .1) + 0.05) 


Q.binned <- Q.modelpredictions %>%
  filter(model == 'Q2') %>%
  group_by(model, modelProbBin) %>%
  tidyboot_mean(correct) 

ggplot(Q.binned, aes(x = modelProbBin, y = mean)) +
    geom_point(aes(size = n), stat = 'identity') +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1) +
    geom_abline(intercept = 0, slope = 1) +
    theme_bw() +
    xlim(0,1) +
    ylim(0,1) +
    theme(aspect.ratio = 1) +
    ylab('human responses') +
    xlab('model predictions')

ggsave('../writing/2019/journal-revision/figures/Exp3/questionerPredictives.pdf', 
        width=14, height = 10, units='cm', useDingbats=FALSE)
```

