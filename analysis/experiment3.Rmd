---
title: "R Notebook"
output: html_notebook
---


```{r}
library(tidyverse)
library(ggthemes)
library(tidyboot)
```

```{r}
d.subj.raw <- read_csv('../data/experiment3/exitSurveyFromMongo.csv') %>%
  filter(iterationName == 'full_sample')
d.goals.raw <- read_csv('../data/experiment3/goalInferenceFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.questions.raw <- read_csv('../data/experiment3/questionFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.answers.raw <- read_csv('../data/experiment3/answerFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 

# nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- union(unique((d.goals.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       union(unique((d.questions.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       unique((d.answers.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid)))
confused <- unique((d.subj.raw %>% filter(confused != 'yes'))$gameid)
nonNative <- unique((d.subj.raw %>% filter(nativeEnglish != 'yes'))$gameid)

badGames <- c(incompleteIDs, nonNative, confused)
d.answers <- left_join(
  d.answers.raw %>% filter(!(gameid %in% badGames)), 
  d.goals.raw %>% filter(!(gameid %in% badGames)) %>% arrange(gameid, trialNum),
  by = c('iterationName', 'gameid', 'trialNum', 'questionNumber', 'trialType', 'cellAskedAbout', 'firstRole', 'gridState')
) %>%
  select(-time.x, -time.y)
d.questions <- d.questions.raw %>% filter(!(gameid %in% badGames))
```

```{r}
cat(length(unique((d.questions.raw %>% group_by(gameid) %>% tally())$gameid)), 'initially recruited')
cat(length(incompleteIDs), 'were incomplete')
cat(length(unique((d.questions %>% group_by(gameid) %>% tally())$gameid)), 'left')
```

```{r}
write_csv(d.answers, '../data/experiment3/answerFromMongo_clean.csv')
write_csv(d.questions, '../data/experiment3/questionFromMongo_clean.csv')
```

## Goal inference 

Note that if you split out by trueGoal, there seems to be a ROW bias (probably bcecause ROW button was first...)
 
We also exclude 'random' style trials for this qualitative analysis since they were all over the place in what goal we predicted people would infer...   
 
```{r}
goal_response_proportions <- d.answers %>% 
  filter(!(trialType %in% c('practice', 'random'))) %>%
  mutate(correct = ifelse(goalResponse == 'not sure', '"not sure"', 
                          ifelse(trueGoal == goalResponse, 'correct', 'incorrect')),
         trialType = ifelse(trialType == 'empty', 'empty', 'pragmatic')) %>%
  filter(questionNumber < 4) 

goal_response_proportions.boot <- map_dfr(seq_len(5000), ~goal_response_proportions %>%
    group_by(trialType, questionNumber) %>%
    mutate(total = length(correct)) %>%
    do(sample_n(., nrow(.), replace=TRUE)) %>%
    group_by(correct, trialType, questionNumber) %>%
    summarize(prop = length(total)/mean(total)) %>%
    mutate(sample_num = .x)) %>%
    group_by(trialType, questionNumber, correct) %>%
    summarize(empirical_prop = mean(prop), ci_upper = ci_upper(prop), ci_lower = ci_lower(prop)) %>%
    ungroup() %>%
    mutate(correct = factor(correct, levels = c('correct', 'incorrect',  '"not sure"')),
          trialType = factor(trialType, levels = c('empty', 'pragmatic')),
          questionNumber = as.character(questionNumber)) %>%
    complete(questionNumber, trialType, correct, 
           fill = list(empirical_prop = 0.002, ci_upper = 0.003,ci_lower =0.001 )) #%>%

ggplot(goal_response_proportions.boot, aes(x = questionNumber, y = empirical_prop, color = correct, fill = correct, group=correct))  +
  #geom_bar(stat = 'identity', position = dodge) +
  geom_line() +
  theme_few() +
  facet_wrap(~ trialType) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2,color = NA) +#, width = 0, position = dodge) +
  scale_fill_colorblind() +
  scale_color_colorblind() +
  
  ylab('% responses') +
  #xlab('')+
  ylim(0, 1) +
  ggtitle("What goal do you think the leader has?")  +
  theme(aspect.ratio = 1)
  
ggsave('../writing/2019/journal-revision/figures/spatialGoalInference.pdf', width=14, height = 10, units='cm')
```

```{r}
goal_response_proportions.boot
goal_response_proportions %>% group_by(questionNumber, trialType, correct) %>% tally()
chisq.test(matrix(c(33, 40, 31, 9, 79, 16), nrow = 3))
```

# Qualitative model predictions

## Questioner

```{r}
qualitativeQ <- read_csv('../modeling/qa/questionFromMongo_fixed.csv') 

qualitativeQ %>%
  filter(trialType == 'blocked') 

qualitativeQ %>%
  group_by(qualitativeTrialType, qualitativeQuestion) %>%
  tidyboot(summary_function = function(x) x %>% summarize(prop = length(total) / mean(total)),
           statistics_functions = function(x) x %>%
           summarise_at(vars(prop), funs(ci_upper, mean, ci_lower)))
```

```{r}
diffScore <- function(data, indices) {
  d <- data[indices,] %>%
    group_by(qualitativeQuestion) %>%
    summarize(count = n()) %>%
    ungroup() %>%
    mutate(prob = count / sum(count)) %>%
    select(-count) %>%
    spread(qualitativeQuestion, prob)
  return(d$pragmatic - d$confusing)
}

bootRes <- qualitativeQ %>% 
  filter(qualitativeTrialType == "ambiguous") %>%
  ungroup() %>%
  boot::boot(diffScore, R = 1000)

estimate <- round(bootRes$t0,2)
lowerDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[4],2)
upperDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[5],2)
cat(paste0(c('\npragmatic-confusing = ', estimate,
             '[', lowerDiffScore, 
             ', ', upperDiffScore, ']')))

```

## Answerer

See how often people *answer* with extra info.

```{r}
d.answers <- read_csv('../modeling/qa/answerFromMongo_fixed.csv') %>%
  group_by(gameid, trialNum) %>%
  mutate(trialType = first(trialType))

answer_info <- d.answers %>% 
  filter(questionNumber == 1) %>%
  filter(!(trialType %in% c('practice'))) %>%
  rowwise() %>%
  mutate(additionalInfo = ifelse(nchar(answer) == 11, 1, 0)) %>%
  mutate(trialType = ifelse(trialType == 'blocked', 'pragmatic', trialType)) %>%
  group_by(trialType) %>%
  tidyboot_mean(additionalInfo) %>% 
  ungroup() %>% 
  mutate(trialType = factor(trialType, levels = c('practice', 'pragmatic', 'empty')))
dodge = position_dodge(.9)
ggplot(answer_info, aes(x = trialType, y = empirical_stat))  +
  geom_bar(stat = 'identity', position = dodge) +
  theme_few() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
  scale_fill_few() +
  ylim(0,1) +
  ylab('% answerers give additional info') +
  xlab('')+
  ggtitle("") 
```

Look specifically at 'blocked' trials -- how do people respond?

```{r}
 d.answers %>% 
  filter(trialType == 'blocked') %>%
  filter(questionNumber == 1)
  rowwise() %>%
    group_by(askedAboutStatus, answerType) %>%
    tally()
```

# Quantitative model comparison

First, since it's pretty expensive to run the model for board in the dataset (without some symmetry collapsing tricks which we may want to do later anyway), we'll score the data under a particular rationality param setting. The relative likelihood of the explicit & pragmatic models shouldn't be affected that much by the rationality (just the absolute fits).

Begin by pruning huge csv of predictions

```{r}
uniqueWorldsInData <- unique(d.answers$underlyingWorld)
uniqueStatesInData <- unique(d.answers$gridState)

d.answer_predictions <- read_csv('../modeling/experiment3/spatialAnswererOutput_HierarchicalVersion.csv')

d.answer_predictions_trimmed <- d.answer_predictions %>% 
  rowwise() %>% 
  filter(world %in% uniqueWorldsInData) %>%
  filter(initState %in% uniqueStatesInData)
  
write_csv(d.answer_predictions_trimmed, '../modeling/experiment3/relevantAnswererOutput.csv')
```

```{r}
library(jsonlite)
out = fromJSON( '../modeling/experiment3/dataAnalysisOut_guessing.txt', flatten = T)
out %>% 
  group_by(questionerType) %>% 
  summarize(prob = reduce(likelihood, sumlogprob) - log(length(likelihood)))
```