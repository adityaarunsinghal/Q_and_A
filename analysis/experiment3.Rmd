---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(tidyverse)
library(ggthemes)
library(tidyboot)
```

```{r}
d.subj.raw <- read_csv('../data/experiment3/exitSurveyFromMongo.csv') %>%
  filter(iterationName == 'full_sample')
d.goals.raw <- read_csv('../data/experiment3/goalInferenceFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.questions.raw <- read_csv('../data/experiment3/questionFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 
d.answers.raw <- read_csv('../data/experiment3/answerFromMongo.csv') %>%
  filter(iterationName == 'full_sample') 

# nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- union(unique((d.goals.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       union(unique((d.questions.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid),
                       unique((d.answers.raw %>% group_by(gameid) %>% 
                           filter(length(unique(trialNum)) != 10))$gameid)))
confused <- unique((d.subj.raw %>% filter(confused != 'yes'))$gameid)
nonNative <- unique((d.subj.raw %>% filter(nativeEnglish != 'yes'))$gameid)

badGames <- c(incompleteIDs, nonNative, confused)
d.goals <- d.goals.raw %>% filter(!(gameid %in% badGames)) %>% arrange(gameid, trialNum)
d.answers <- d.answers.raw %>% filter(!(gameid %in% badGames))
d.questions <- d.questions.raw %>% filter(!(gameid %in% badGames))
```

```{r}
cat(length(unique((d.questions.raw %>% group_by(gameid) %>% tally())$gameid)), 'initially recruited')
cat(length(incompleteIDs), 'were incomplete')
cat(length(unique((d.questions %>% group_by(gameid) %>% tally())$gameid)), 'left')
```

```{r}
write_csv(d.answers, '../data/experiment3/answerFromMongo_clean.csv')
write_csv(d.questions, '../data/experiment3/questionFromMongo_clean.csv')
```

Note that if you split out by trueGoal, there seems to be a ROW bias (probably bcecause ROW button was first...)
 
We also exclude 'random' style trials for this analysis since they were all over the place in what goal we predicted people would infer...   
 
```{r}
goal_response_proportions <- d.goals %>% 
  filter(!(trialType %in% c('practice', 'random'))) %>%
  mutate(correct = ifelse(goalResponse == 'not sure', '"not sure"', 
                          ifelse(trueGoal == goalResponse, 'correct', 'incorrect')),
         trialType = ifelse(trialType == 'empty', 'empty', 'pragmatic')) %>%
  filter(questionNumber < 4) 

goal_response_proportions.boot <- map_dfr(seq_len(1000), ~goal_response_proportions %>%
    group_by(trialType, questionNumber) %>%
    mutate(total = length(correct)) %>%
    do(sample_n(., nrow(.), replace=TRUE)) %>%
    group_by(correct, trialType, questionNumber) %>%
    summarize(prop = length(total)/mean(total)) %>%
    mutate(sample_num = .x)) %>%
    group_by(trialType, questionNumber, correct) %>%
    summarize(empirical_prop = mean(prop), ci_upper = ci_upper(prop), ci_lower = ci_lower(prop)) %>%
    ungroup() %>%
    mutate(correct = factor(correct, levels = c('correct', 'incorrect',  '"not sure"')),
          trialType = factor(trialType, levels = c('empty', 'pragmatic')),
          questionNumber = as.character(questionNumber)) %>%
    complete(questionNumber, trialType, correct, 
           fill = list(empirical_prop = 0.002, ci_upper = 0.003,ci_lower =0.001 )) #%>%

dodge = position_dodge(.9)
ggplot(goal_response_proportions.boot, aes(x = questionNumber, y = empirical_prop, color = correct, fill = correct, group=correct))  +
  #geom_bar(stat = 'identity', position = dodge) +
  geom_line() +
  theme_few() +
  facet_wrap(~ trialType) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2,color = NA) +#, width = 0, position = dodge) +
  scale_fill_colorblind() +
  scale_color_colorblind() +
  
  ylab('% responses') +
  #xlab('')+
  ylim(0, 1) +
  ggtitle("What goal do you think the leader has?")  +
  theme(aspect.ratio = 1)
  
ggsave('../writing/2019/journal-revision/figures/spatialGoalInference.pdf', width=14, height = 10, units='cm')
```

```{r}
goal_response_proportions %>% group_by(questionNumber, trialType, correct) %>% tally()
chisq.test(matrix(c(33, 40, 31, 9, 79, 16), nrow = 3))
```

# Qualitative model predictions

## Questioner

```{r}
qualitativeQ <- read_csv('../modeling/qa/questionFromMongo_qualitative.csv') %>%
  filter(qualitativeTrialType != 'other') %>%
  group_by(qualitativeTrialType) %>%
  mutate(total = length(qualitativeQuestion)) 

qualitativeQ %>%
  group_by(qualitativeTrialType, qualitativeQuestion) %>%
  tidyboot(summary_function = function(x) x %>% summarize(prop = length(total) / mean(total)),
           statistics_functions = function(x) x %>%
           summarise_at(vars(prop), funs(ci_upper, mean, ci_lower)))
```

```{r}
diffScore <- function(data, indices) {
  d <- data[indices,] %>%
    group_by(qualitativeQuestion) %>%
    summarize(count = n()) %>%
    ungroup() %>%
    mutate(prob = count / sum(count)) %>%
    select(-count) %>%
    spread(qualitativeQuestion, prob)
  return(d$pragmatic - d$confusing)
}

bootRes <- qualitativeQ %>% 
  filter(qualitativeTrialType == "ambiguous") %>%
  ungroup() %>%
  boot::boot(diffScore, R = 1000)

estimate <- round(bootRes$t0,2)
lowerDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[4],2)
upperDiffScore <- round(boot::boot.ci(bootRes, type = "perc")$percent[5],2)
cat(paste0(c('\npragmatic-confusing = ', estimate,
             '[', lowerDiffScore, 
             ', ', upperDiffScore, ']')))

```

## Answerer

See how often people *answer* with extra info

```{r}
answer_info <- d.answers %>% 
  filter(questionNumber == 1) %>%
  filter(!(trialType %in% c('random'))) %>%
  rowwise() %>%
  mutate(additionalInfo = ifelse(nchar(answer) == 11, 1, 0)) %>%
  mutate(trialType = ifelse(trialType == 'blocked', 'pragmatic', trialType)) %>%
  group_by(trialType) %>%
  tidyboot_mean(additionalInfo) %>% 
  ungroup() %>% 
  mutate(trialType = factor(trialType, levels = c('practice', 'pragmatic', 'empty')))

ggplot(answer_info, aes(x = trialType, y = empirical_stat))  +
  geom_bar(stat = 'identity', position = dodge) +
  theme_few() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
  scale_fill_few() +
  ylim(0,1) +
  ylab('% answerers give additional info') +
  xlab('')+
  ggtitle("") 
```

# Quantitative model comparison

First, since it's pretty expensive to run the model for board in the dataset (without some symmetry collapsing tricks which we may want to do later anyway), we'll score the data under a particular rationality param setting. The relative likelihood of the explicit & pragmatic models shouldn't be affected that much by the rationality (just the absolute fits).

Begin by pruning huge csv of predictions

```{r}
uniqueWorldsInData <- unique(d.answers$underlyingWorld)
uniqueStatesInData <- unique(d.answers$gridState)

d.answer_predictions <- read_csv('../modeling/experiment3/spatialAnswererOutput_HierarchicalVersion.csv')

d.answer_predictions_trimmed <- d.answer_predictions %>% 
  rowwise() %>% 
  filter(world %in% uniqueWorldsInData) %>%
  filter(initState %in% uniqueStatesInData)
  
write_csv(d.answer_predictions_trimmed, '../modeling/experiment3/relevantAnswererOutput.csv')
```

```{r}
library(jsonlite)
out = fromJSON( '../modeling/experiment3/dataAnalysisOut_guessing.txt', flatten = T)
out %>% 
  group_by(questionerType) %>% 
  summarize(prob = reduce(likelihood, sumlogprob) - log(length(likelihood)))
```