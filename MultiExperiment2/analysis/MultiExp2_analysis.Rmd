---
title: "MultiExp2_analysis"
output: html_document
---

Set wd

```{r}
library(knitr)
opts_knit$set(root.dir = '/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/')
setwd("/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/")
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(MultinomialCI)
```

Experiment data analysis
--------------------------

Load data, pull in game ids from the mturk info dump

```{r}
ps = read.csv("./data/MultiExp2_compiled-subject_information.csv",
              sep = ',', header = TRUE)
mturk = read.csv("./data/MultiExp2_compiled-mturk.csv",
                 sep=',', header= TRUE) %>%
  mutate(gameID = Answer.id) %>%
  select(workerid, gameID)
d = read.csv("./data/MultiExp2_compiled-trials.csv",  
             sep = ',', header = TRUE) %>%
  right_join(mturk, by = 'workerid')
```

Filter out participants who didn't complete all 12 trials or didn't list english as their native language, then get rid of duplicate games

```{r}
english_ps <- (ps %>% 
               filter(nativeEnglish == "yes"))$workerid
cat("We removed", length(ps$workerid) - length(english_ps), "ps due to language")
#trialsCompleted = d %>% count(workerid) %>% mutate(numCompleted = n)
filteredD = d %>% filter(workerid %in% english_ps) %>% 
       filter(answer != "NA") 
completedGames = (filteredD %>% group_by(gameID) %>% 
                    count(gameID) %>% mutate(numCompleted = n) %>% 
                    filter(numCompleted == 24))$gameID
cat("We removed", length(unique(d$workerid)) - length(completedGames)*2, "games total")
d <- filteredD %>% filter(gameID %in% completedGames) %>% 
     distinct(domain, goal, question, guess, answer, type, gameID)
#     filter(numCompleted == 12) %>% 
#     distinct(domain, goal, question, guess, answer, type, gameID)    
# make counts 

  
    #group_by(gameID) %>% 
#     count(workerid) %>%
#     filter(n == 2)# %>%
```

In order to compare different items in a convenient way, we're going to map the questions and answers to the corresponding node positions in the hierarchy. So, in 'branching' trials, for example, 'dalmatian', 'mansion', 'carrot', and 'couch' would all be treated the same.


```{r}

source("~/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/analysis/analysisHelpers.R")
mapWordsToNodes <- function(d) {
  answerNodes = c()
  questionNodes = c()
  goalNodes = c()
  for(i in 1:length(d$workerid)) {
    answerNodes <- append(answerNodes, mapAnswer(d[i,]))
    questionNodes <- append(questionNodes, mapQuestion(d[i,]))
    goalNodes <- append(goalNodes, mapGoal(d[i,]))
  }
  d$goalNodes = goalNodes
  d$answerNodes = answerNodes
  d$questionNodes = questionNodes
  return(d)
}

d <- mapWordsToNodes(d)
```

Tidy up questioner data...

```{r}
d_q_temp = d %>% 
      mutate(response = ordered(questionNodes, levels = c("Q1", "Q2", "Q3", "Q4"))) %>%
      mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
      group_by(domain, type, goal) %>%
      mutate(group_size = n()) %>%
      group_by(domain, type, goal, response) %>% # collapse over participants
      summarize(count = n(), empProb = n() / mean(group_size), group_size = mean(group_size))%>%
      select(goal, type, response, count, group_size, empProb)

# Hadley Wickham hasn't fixed summarize to take drop = F yet, so we have to re-insert rows with p = 0
d_q = d_q_temp %>%
      right_join(expand.grid(domain = levels(d_q_temp$domain),
                             type = levels(d_q_temp$type),
                             response = levels(d_q_temp$response),
                             goal = levels(d_q_temp$goal))) %>%
      do(mutate(., count = ifelse(is.na(count), 0, count))) %>%
      do(mutate(., group_size = ifelse(is.na(group_size), 0, group_size))) %>%
      do(mutate(., empProb = ifelse(is.na(empProb), 0, empProb)))

# get confidence intervals in the stupidest way possible...
# qud_levels = unique(d_q$goal)
# cis = rbind(multinomialCI(subset(d_q, goal == qud_levels[1])$count, .05),
#             multinomialCI(subset(d_q, goal == qud_levels[2])$count, .05),
#             multinomialCI(subset(d_q, goal == qud_levels[3])$count, .05),
#             multinomialCI(subset(d_q, goal == qud_levels[4])$count, .05))
# colnames(cis) <- c("lower_ci", "upper_ci")
# d_q = cbind(d_q, cis)
```

Tidy up answerer data...

```{r}
d_a = d %>% 
      mutate(response = ordered(answerNodes,
                                levels=c("A1","A2","A3","A4"))) %>%
      mutate(utterance = ordered(questionNodes,
                                 levels = c("Q1", "Q2", "Q3", "Q4"))) %>%
      group_by(domain, type, utterance) %>%
      mutate(group_size = n()) %>%
      group_by(domain, type, utterance, response) %>% # collapse over participants
      summarize(count = n(), empProb = n() / mean(group_size),
                group_size = mean(group_size))%>%
      select(domain, type, utterance, response, group_size, empProb, count)

# Hadley Wickham hasn't fixed summarize to take drop = F yet, so we have to re-insert rows with p = 0
d_a = d_a %>% 
      full_join(expand.grid(domain = levels(d_a$domain),
                             type = levels(d_a$type), 
                             response  = levels(d_a$response),
                             utterance = levels(d_a$utterance))) %>%
       do(mutate(., count = ifelse(is.na(count), 0, count),
                    countp1 = ifelse(is.na(count), .25, count + .25))) %>%
       do(mutate(., empProb = ifelse(is.na(empProb), 0, empProb))) %>%
       do(mutate(., group_size = ifelse(is.na(group_size), 0, group_size))) %>%
       # compute confidence intervals
       group_by(domain, type, utterance) %>% 
       do(mutate(., lower_ci = multinomialCI(countp1, 0.05)[,1], 
                    upper_ci = multinomialCI(countp1, 0.05)[,2]))
```

How well do the different domains correlate?
----------------------

```{r}
col1 = subset(d_q, domain == "animals")$empProb
col2 = subset(d_q, domain == "places")$empProb
col3 = subset(d_q, domain == "plants")$empProb
col4 = subset(d_q, domain == "artifact")$empProb
corData_q = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cor(corData_q)

col1 = subset(d_a, domain == "animals")$empProb
col2 = subset(d_a, domain == "places")$empProb
col3 = subset(d_a, domain == "plants")$empProb
col4 = subset(d_a, domain == "artifact")$empProb
corData_a = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cor(corData_a)
```



Fitting rationality parameters
------------------------------

First, we define this function that takes a data frame and computes which parameter values optimize the correlation between model and data.

```{r}
# Expect two columns that end with _prob (i.e. emp_prob and model_prob)
optimalFit <- function(data, equal = FALSE) {
  prob_correlation <- data %>%
#     group_by(modelLevel, rationality, goal, response, modelProb) %>%
#     summarise(empProb = mean(empProb)) %>%
    group_by(modelLevel, rationality) %>%
    filter(rationality > 1) %>%
    summarise(correlation = cor(modelProb, empProb, method = 'pearson')) %>%
    mutate(m = max(correlation)) %>%
    ungroup() %>%
    filter(m == correlation) %>%
    mutate(maximizingR = rationality) %>%
    group_by(modelLevel, correlation) %>%
    summarize(maximizingR = min(maximizingR)) %>%
    select(modelLevel, maximizingR, correlation)
  # add literal back in w/ correlation = NA
  
  prob_correlation <- rbind(prob_correlation, c('literal', 1.0, NA))

print(prob_correlation)
  return(data %>% join(prob_correlation) %>% 
         filter(rationality == maximizingR))
}
```

Now we import the literal answer fits and tidy them up to eventually be joined
```{r}
ansFits = optimalFit(inner_join(d_a, read.csv("analysis/predictions/answererPredictions.csv", sep = ','))) %>%
  select(domain, modelLevel, type, utterance, response, empProb, rationality, modelProb)
```

Now that each of these data sets is in a nice format, we can join them all together and plot their fits:

```{r}
# join them all together
all_ans = d_a %>% 
  inner_join(ansFits, by = c('domain', 'type', 'utterance', 'response', 'empProb')) %>%
#  group_by(type, modelLevel, utterance, response, modelProb, rationality) %>%
#  summarise(meanEmpProb = mean(emp_prob)) %>%
#  ungroup() %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, utterance, response, empProb, modelLevel, modelProb) 
  #distinct(utterance, response, model_level, model_prob)

# Since they won't let us annotate nicely...
  
labelDataFrame = all_ans %>% 
   group_by(type, modelLevel) %>% 
   summarise(correlation = paste("r =", round(cor(empProb, 
                                                  modelProb, method = 'pearson'), 2)))

answer_plots = (ggplot(all_ans, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_text(angle=90, vjust=1))
  + xlab("Model predicted probability")
  + ylim(0,1)
  + ylab("")
  + geom_point(aes(colour = domain))
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + scale_x_continuous(lim = c(0,1), breaks=c(0,.5,1))
  + ggtitle("Answerers")
  + geom_smooth(method = "lm")
  + facet_grid(type ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = labelDataFrame$modelLevel,
                              type = labelDataFrame$type,
                              lab = labelDataFrame$correlation)))
answer_plots               
```

Now we import the literal questioner fits and tidy them up to eventually be joined

```{r}
questFits = optimalFit(inner_join(d_q, read.csv("analysis/predictions/questionerPredictions.csv", sep = ','))) %>%
  select(type, domain, modelLevel, goal, response, empProb, modelProb)
```

Now make the questioner plot

```{r}
# join them all together
all_qs = d_q %>% 
  inner_join(questFits, by = c('type', 'domain', 'goal', 'response', 'empProb')) %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, goal, response, empProb, modelLevel, modelProb) 
  #distinct(type, domain, goal, response, modelLevel, modelProb)

# Since they won't let us annotate nicely...
  
labelDataFrame = all_qs %>% 
   group_by(type, modelLevel) %>% 
   summarise(correlation = paste("r =", round(cor(empProb, 
                                                  modelProb, method = 'pearson'), 2)))

#jpeg(filename="../writing/2015/cogsci/questionerFits.jpeg")
question_plots = (ggplot(all_qs, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_text(angle=90, vjust=1))
#          axis.text.x = element_blank(), axis.ticks = element_blank())
#          plot.margin=unit(c(1,1,-1,1), "cm"))
  + ylim(0,1)
  + xlab("Model Predicted Probability")
  + ylab("Empirical Probability")
  + geom_point(aes(colour = domain))
  + ggtitle("Questioner model fits")
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + geom_smooth(method = "lm")
  + facet_grid(type ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = labelDataFrame$modelLevel,
                              type = labelDataFrame$type,
                              lab = labelDataFrame$correlation)))
question_plots               
#dev.off()
```

Put these next to each other
```{r}
pdf("model_fits_grid.pdf")
grid.newpage()
grid.draw(rbind(ggplotGrob(question_plots), ggplotGrob(answer_plots), size="last"))
grid.draw(textGrob("Empirical probability", rot = 90, vjust = 1, 
                   x = unit(0.01, "npc"), y = unit(0.5, "npc"),
                   gp=gpar(fontsize=20)))
dev.off()
```

Model + data bar plots
----------------------

Plot for pragmatic questioner. These bar graphs will help show what our model is getting right and what it's getting wrong.

```{r}
# first, average over domains
collapsed_q = d_q %>% 
  group_by(goal, type, response) %>% 
  summarize(emperical = sum(count)/sum(group_size)) %>%
  do(mutate(., emperical = ifelse(is.nan(emperical), 0, emperical)))
pragModelPreds = questFits %>% 
  group_by(goal, type, modelLevel, response) %>% 
  summarize(model = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")

plottableQ = collapsed_q %>% inner_join(pragModelPreds, by = c('goal', 'type', 'response')) %>%
  select(goal, type, response, emperical, model) %>%
  gather(src, prob, emperical, model)

pdf('exp4QuestResults.pdf')
qplot(x = response, y = prob, fill = src,
      data = plottableQ, 
      facets = type ~ goal, 
      main = "Pragmatic Questioner",
      geom = 'bar', stat = 'identity', position = 'dodge')
dev.off()

```


```{r}
# first, average over domains
collapsed_a = d_a %>% 
  group_by(utterance, type, response) %>% 
  summarize(emperical = sum(count)/sum(group_size)) %>%
  do(mutate(., emperical = ifelse(is.nan(emperical), 0, emperical)))
pragModelPreds = ansFits %>% 
  group_by(utterance, type, modelLevel, response) %>% 
  summarize(model = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")

plottableA = collapsed_a %>% inner_join(pragModelPreds, by = c('utterance', 'type', 'response')) %>%
  select(utterance, type, response, emperical, model) %>%
  gather(src, prob, emperical, model)

pdf('exp4AnsResults.pdf')
qplot(x = response, y = prob, fill = src,
      data = plottableA, 
      facets = type ~ utterance, 
      main = "Pragmatic Answerer",
      geom = 'bar', stat = 'identity', position = 'dodge')
dev.off()
```
Now, plot for answerer model:

```{r}
fitted_data = (prag_ans_fits %>% rename(model_prob = pragmatic_prob)) 
plot_title = "Pragmatic_Answerer"
#fitted_data = (exp_ans_fits %>% rename(model_prob = explicit_prob)) 
#plot_title = "Explicit_Answerer"

new_labels = as.factor(sapply(X = fitted_data$utterance, FUN = function(v) {return(paste("utterance:", v))}))
fitted_data$facet_label = ordered(new_labels, 
                              levels = c("utterance: dalmatian", "utterance: dog", 
                                         "utterance: pet","utterance: animal"))
a_comparison <- fitted_data %>% 
  select(utterance, response, emp_prob, upper_ci, lower_ci, model_prob, facet_label) %>%
  rename(empirical = emp_prob, model = model_prob) %>%
  gather(src, prob, empirical, model) 

a_comparison[a_comparison$src == "model",]$upper_ci = a_comparison[a_comparison$src == "model",]$prob
a_comparison[a_comparison$src == "model",]$lower_ci = a_comparison[a_comparison$src == "model",]$prob
name = paste(plot_title, ".pdf", sep = '')
pdf(name)
dodge <- position_dodge(width=0.9)
g4<-(ggplot(a_comparison, aes(x=response, y = prob, fill=src)) 
     + geom_bar(stat='identity', position=position_dodge())
     + geom_errorbar(aes(ymax = upper_ci, ymin = lower_ci), 
                    position=dodge, width = .25)
     + scale_fill_grey()
     + theme_bw(base_size = 20)
     + ggtitle(plot_title)
     + theme(axis.text.x = element_text(angle=90, vjust=1))
     + facet_wrap(~facet_label))
g4
dev.off()
```

quick plots

```{r}
qplot(x = response, y = emp_prob,
      data = subset(d_a, domain == "places" & type == "overlapping"), 
      facets = type ~ utterance, 
      geom = 'bar', stat = 'identity', position = 'dodge')
```

full answer

```{r}
qplot(x = response, y = emp_prob, fill = domain,
      data = d_a, facets = type ~ utterance, 
      geom = 'bar', stat = 'identity',  position = 'dodge')
```

full question

```{r}
qplot(x = response, y = empProb, fill = domain,
      data = d_q, facets = type ~ goal, 
      geom = 'bar', stat = 'identity',  position = 'dodge')
```

zoom in on overlapping condition

```{r}

collapsed_q = d_q %>% 
  group_by(goal,response,type) %>%
  filter(type == "overlapping") %>%
  filter(domain != "places") %>%
  summarize(total = sum(count)) %>%
  group_by(goal) %>%
  mutate(group_size = sum(total)) %>%
  ungroup() %>%
  mutate(emperical = total/group_size) %>%
  do(mutate(., emperical = ifelse(is.nan(emperical), 0, emperical)))
pragModelPreds = questFits %>% 
    filter(domain != "places") %>%
  group_by(goal, type, modelLevel, response) %>% 
  summarize(pragmaticModel = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")
expModelPreds = questFits %>% 
  filter(domain != "places") %>%
  group_by(goal, type, modelLevel, response) %>% 
  summarize(explicitModel = mean(modelProb)) %>% 
  filter(modelLevel == "explicit")

plottableQ = collapsed_q %>% 
  inner_join(pragModelPreds, by = c('goal', 'type', 'response')) %>%
  inner_join(expModelPreds, by = c('goal', 'type', 'response')) %>%
  select(goal, type, response, emperical, pragmaticModel, explicitModel) %>%
  gather(src, prob, emperical, pragmaticModel, explicitModel) %>%
  filter(goal == "G2")

(qplot(x = response, y = prob, fill = src,
      data = plottableQ, 
      facets = type ~ goal, 
      main = "Overlapping model comparison",
      geom = 'bar', stat = 'identity', position = 'dodge')
 + geom_hline(yintercept = 0.48)
 + theme_bw())

```