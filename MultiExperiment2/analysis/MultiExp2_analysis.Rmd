---
title: "MultiExp2_analysis"
output: html_document
---

Set wd

```{r}
library(knitr)
opts_knit$set(root.dir = '/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/')
setwd("/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/")
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
#library(MultinomialCI)
library(boot)
```

Experiment data analysis
--------------------------

Load data, pull in game ids from the mturk info dump

```{r}
ps = read.csv("./data/MultiExp2_compiled-subject_information.csv",
              sep = ',', header = TRUE)
mturk = read.csv("./data/MultiExp2_compiled-mturk.csv",
                 sep=',', header= TRUE) %>%
  mutate(gameID = Answer.id) %>%
  select(workerid, gameID)
d = read.csv("./data/MultiExp2_compiled-trials.csv",  
             sep = ',', header = TRUE) %>%
  right_join(mturk, by = 'workerid')
```

Filter out participants who didn't complete all 12 trials or didn't list english as their native language, then get rid of duplicate games

```{r}
english_ps <- (ps %>% 
               filter(nativeEnglish == "yes"))$workerid
cat("We removed", length(ps$workerid) - length(english_ps), "ps due to language")
#trialsCompleted = d %>% count(workerid) %>% mutate(numCompleted = n)
filteredD = d %>% filter(workerid %in% english_ps) %>% 
       filter(answer != "NA") 
completedGames = (filteredD %>% group_by(gameID) %>% 
                    count(gameID) %>% mutate(numCompleted = n) %>% 
                    filter(numCompleted == 24))$gameID
cat("We removed", length(unique(d$workerid)) - length(completedGames)*2, "games total")
d <- filteredD %>% filter(gameID %in% completedGames) %>% 
     distinct(domain, goal, question, guess, answer, type, gameID)
#     filter(numCompleted == 12) %>% 
#     distinct(domain, goal, question, guess, answer, type, gameID)    
# make counts 

  
    #group_by(gameID) %>% 
#     count(workerid) %>%
#     filter(n == 2)# %>%
```

In order to compare different items in a convenient way, we're going to map the questions and answers to the corresponding node positions in the hierarchy. So, in 'branching' trials, for example, 'dalmatian', 'mansion', 'carrot', and 'couch' would all be treated the same.


```{r}
source("~/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/analysis/analysisHelpers.R")
mapWordsToNodes <- function(d) {
  answerNodes = c()
  questionNodes = c()
  goalNodes = c()
  for(i in 1:length(d$workerid)) {
    answerNodes <- append(answerNodes, mapAnswer(d[i,]))
    questionNodes <- append(questionNodes, mapQuestion(d[i,]))
    goalNodes <- append(goalNodes, mapGoal(d[i,]))
  }
  d$goalNodes = goalNodes
  d$answerNodes = answerNodes
  d$questionNodes = questionNodes
  return(d)
}

d <- mapWordsToNodes(d)
```

We're also going to estimate empirical probabilities for each response, conditioned on the domain, type, and goal of the trial. To get confidence intervals for these estimates, we'll use the bootstrap.

```{r}
## This function takes a raw questioner data set ('q') or an answerer dataset ('a')
## and estimates the emperical probability of each response
##
## Note that this currently does not collapse over any aspect of the experiment.
## Below, the function getProbsAndCIs does so.
getProbs <- function(data, QorA, R) {
  if(QorA == "q") {
    tempData <- data %>% group_by(domain, type, goal, response) %>% 
         summarize(count = n()) %>%
         right_join(expand.grid(response = levels(data$response),
                                type = levels(factor(data$type)),
                                domain = levels(factor(data$domain)),
                                goal = levels(factor(data$goal)))) 
  } else if(QorA == "a") {
    tempData <- data %>% group_by(domain, type, utterance, response) %>% 
         summarize(count = n()) %>%
         right_join(expand.grid(response = levels(data$response),
                                type = levels(factor(data$type)),
                                domain = levels(factor(data$domain)),
                                utterance = levels(factor(data$utterance)))) 
  } else {
    stop(cat("Did not specify Q or A:", QorA))
  }
  
  outputData <- tempData %>% 
     do(mutate(., count = ifelse(is.na(count), 0, count),
                  empProb = count / sum(count),
                  groupSize = sum(count)))
  return(outputData)
}
```

Tidy up questioner data...

```{r}
d_q = d %>% 
      mutate(response = ordered(questionNodes, levels = c("Q1","Q2","Q3","Q4"))) %>%
      mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
      group_by(domain, type, goal) %>%
      do(getProbs(data = ., QorA = 'q')) %>%
      select(goal, type, response, domain, count,
             groupSize, empProb)
```

Tidy up answerer data...

```{r}
d_a = d %>% 
      mutate(response = ordered(answerNodes,levels=c("A1","A2","A3","A4"))) %>%
      mutate(utterance = ordered(questionNodes,levels = c("Q1","Q2","Q3","Q4"))) %>%
      group_by(domain, type, utterance) %>%
      do(getProbs(data = ., QorA = 'a')) %>%
      select(utterance, type, response, domain, count,
             groupSize, empProb)
```

How well do the different domains correlate?
----------------------

```{r}
col1 = subset(d_q, domain == "animals")$empProb
col2 = subset(d_q, domain == "places")$empProb
col3 = subset(d_q, domain == "plants")$empProb
col4 = subset(d_q, domain == "artifact")$empProb
corData_q = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cor(corData_q)

col1 = subset(d_a, domain == "animals")$empProb
col2 = subset(d_a, domain == "places")$empProb
col3 = subset(d_a, domain == "plants")$empProb
col4 = subset(d_a, domain == "artifact")$empProb
corData_a = data.frame(animal = col1, place = col2, plant = col3, artifact = col4)
cor(corData_a)
```



Fitting rationality parameters
------------------------------

First, we define this function that takes a data frame and computes which parameter values optimize the correlation between model and data.

```{r}
# Expect two columns that end with _prob (i.e. emp_prob and model_prob)
optimalFit <- function(data, equal = FALSE) {
  prob_correlation <- data %>%
#     group_by(modelLevel, rationality, goal, response, modelProb) %>%
#     summarise(empProb = mean(empProb)) %>%
    group_by(modelLevel, rationality) %>%
    filter(rationality > 1) %>%
    summarise(correlation = cor(modelProb, empProb, method = 'pearson')) %>%
    mutate(m = max(correlation)) %>%
    ungroup() %>%
    filter(m == correlation) %>%
    mutate(maximizingR = rationality) %>%
    group_by(modelLevel, correlation) %>%
    summarize(maximizingR = min(maximizingR)) %>%
    select(modelLevel, maximizingR, correlation)
  # add literal back in w/ correlation = NA
  
  prob_correlation <- rbind(prob_correlation, c('literal', 1.0, NA))

print(prob_correlation)
  return(data %>% join(prob_correlation) %>% 
         filter(rationality == maximizingR))
}
```

Now we import the literal answer fits and tidy them up to eventually be joined
```{r}
ansFits = optimalFit(inner_join(d_a, read.csv("analysis/predictions/answererPredictions.csv", sep = ','))) %>%
  select(domain, modelLevel, type, utterance, response, empProb, rationality, modelProb)
```

Now that each of these data sets is in a nice format, we can join them all together and plot their fits:

```{r}
# join them all together
all_ans = d_a %>% 
  inner_join(ansFits, by = c('domain', 'type', 'utterance', 'response', 'empProb')) %>%
#  group_by(type, modelLevel, utterance, response, modelProb, rationality) %>%
#  summarise(meanEmpProb = mean(emp_prob)) %>%
#  ungroup() %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, utterance, response, empProb, modelLevel, modelProb) 
  #distinct(utterance, response, model_level, model_prob)

# Since they won't let us annotate nicely...
  
labelDataFrame = all_ans %>% 
   group_by(type, modelLevel) %>% 
   summarise(correlation = paste("r =", round(cor(empProb, 
                                                  modelProb, method = 'pearson'), 2)))

answer_plots = (ggplot(all_ans, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_text(angle=90, vjust=1))
  + xlab("Model predicted probability")
  + ylim(0,1)
  + ylab("")
  + geom_point(aes(colour = domain))
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + scale_x_continuous(lim = c(0,1), breaks=c(0,.5,1))
  + ggtitle("Answerers")
  + geom_smooth(method = "lm")
  + facet_grid(type ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = labelDataFrame$modelLevel,
                              type = labelDataFrame$type,
                              lab = labelDataFrame$correlation)))
answer_plots               
```

Now we import the literal questioner fits and tidy them up to eventually be joined

```{r}
questFits = optimalFit(inner_join(d_q, read.csv("analysis/predictions/questionerPredictions.csv", sep = ','))) %>%
  select(type, domain, modelLevel, goal, response, empProb, modelProb)
```

Now make the questioner plot

```{r}
# join them all together
all_qs = d_q %>% 
  inner_join(questFits, by = c('type', 'domain', 'goal', 'response', 'empProb')) %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, goal, response, empProb, modelLevel, modelProb) 
  #distinct(type, domain, goal, response, modelLevel, modelProb)

# Since they won't let us annotate nicely...
  
labelDataFrame = all_qs %>% 
   group_by(type, modelLevel) %>% 
   summarise(correlation = paste("r =", round(cor(empProb, 
                                                  modelProb, method = 'pearson'), 2)))

#jpeg(filename="../writing/2015/cogsci/questionerFits.jpeg")
question_plots = (ggplot(all_qs, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_text(angle=90, vjust=1))
#          axis.text.x = element_blank(), axis.ticks = element_blank())
#          plot.margin=unit(c(1,1,-1,1), "cm"))
  + ylim(0,1)
  + xlab("Model Predicted Probability")
  + ylab("Empirical Probability")
  + geom_point(aes(colour = domain))
  + ggtitle("Questioner model fits")
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + geom_smooth(method = "lm")
  + facet_grid(type ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = labelDataFrame$modelLevel,
                              type = labelDataFrame$type,
                              lab = labelDataFrame$correlation)))
question_plots               
#dev.off()
```

Put these next to each other
```{r}
# pdf("model_fits_grid.pdf")
# grid.newpage()
# grid.draw(rbind(ggplotGrob(question_plots), ggplotGrob(answer_plots), size="last"))
# grid.draw(textGrob("Empirical probability", rot = 90, vjust = 1, 
#                    x = unit(0.01, "npc"), y = unit(0.5, "npc"),
#                    gp=gpar(fontsize=20)))
# dev.off()
```

Model + data bar plots
----------------------

Plot for pragmatic questioner. These bar graphs will help show what our model is getting right and what it's getting wrong.

```{r}
 # called a bunch of times on questioner data set to bootstrap CI
Qprobs <- function(data, indices) {
  d <- data[indices,] # allows boot to select sample 
  pseudocount <- 0#runif(1, max = .25)
  c <- d %>% 
       group_by(type, goal, response) %>% 
       summarize(count = n()) %>%
       right_join(expand.grid(response = levels(d$response),
                              type = levels(factor(d$type)),
                              goal = levels(factor(d$goal)))) %>% 
       do(mutate(., countp1 = ifelse(is.na(count), 
                                     pseudocount, count + pseudocount),
                    count = ifelse(is.na(count), 0, count),
                   empProb = countp1 / sum(countp1)))
  return(c$empProb)
}

# called a bunch of times on answerer data set to bootstrap CI
Aprobs <- function(data, indices) {
  d <- data[indices,] # allows boot to select sample 
  pseudocount <- 0#runif(1, max = .25)
  c <- d %>% 
       group_by(type, utterance, response) %>% 
       summarize(count = n()) %>%
       right_join(expand.grid(response = levels(d$response),
                              type = levels(factor(d$type)),
                              utterance = levels(factor(d$utterance)))) %>% 
       do(mutate(., countp1 = ifelse(is.na(count), pseudocount, count + pseudocount),
                    count = ifelse(is.na(count), 0, count),
                    empProb = countp1 / sum(countp1)))  
  return(c$empProb)
} 

## This function takes a raw questioner data set ('q') or an answerer dataset ('a')
## and estimates the emperical probability of each response, along with 
## bootstrapped confidence intervals
##
## Note that this currently does not collapse over any aspect of the experiment.
## If we want to do that in the future, will just group by fewer things before 
## summarizing
getProbsAndCIs <- function(data, QorA, R) {
  if(QorA == "q") {
    tempData <- data %>% group_by(type, goal, response) %>% 
         summarize(count = n()) %>%
         right_join(expand.grid(response = levels(data$response),
                                type = levels(factor(data$type)),
                                goal = levels(factor(data$goal)))) 
  } else if(QorA == "a") {
    tempData <- data %>% group_by(type, utterance, response) %>% 
         summarize(count = n()) %>%
         right_join(expand.grid(response = levels(data$response),
                                type = levels(factor(data$type)),
                                utterance = levels(factor(data$utterance)))) 
  } else {
    stop(cat("Did not specify Q or A:", QorA))
  }
  
  outputData <- tempData %>% 
     do(mutate(., count = ifelse(is.na(count), 0, count),
                  empProb = count / sum(count),
                  groupSize = sum(count)))

  print(outputData)
  # Get confidence intervals
  print(QorA)
  if(QorA == "q") {
    bootObj <-  boot(data = data,statistic = Qprobs,R=R)
  } else {
    bootObj <-  boot(data = data,statistic = Aprobs,R=R)
  }

  print(bootObj)
  upper_ci <- c()
  lower_ci <- c()
  for(i in 1:4) {
    lower = boot.ci(bootObj, index = i, type = "perc")$percent[4]
    upper = boot.ci(bootObj, index = i, type = "perc")$percent[5]
    if(is.null(lower) | is.null(upper)) {
      lower = outputData$empProb[i]
      upper = outputData$empProb[i]
    }
    lower_ci = append(lower_ci, lower)
    upper_ci = append(upper_ci, upper)
  }

  outputData$lower_ci = lower_ci
  outputData$upper_ci = upper_ci

  return(outputData)
}
```

```{r}
# first, average over domains
collapsed_q <- d %>% 
    mutate(response = ordered(questionNodes, levels = c("Q1","Q2","Q3","Q4"))) %>%
    mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
    group_by(type, goal) %>%
    do(getProbsAndCIs(data = ., QorA = 'q', R = 1000)) %>%
    mutate(empirical = empProb) %>%
    select(goal, type, response, count, lower_ci, upper_ci,
           groupSize, empirical)

pragModelPreds = questFits %>% 
  group_by(goal, type, modelLevel, response) %>% 
  summarize(model = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")

plottableQ = collapsed_q %>% inner_join(pragModelPreds, by = c('goal', 'type', 'response')) %>%
  select(goal, type, response, empirical, model, lower_ci, upper_ci) %>%
  gather(src, prob, empirical, model) %>% 
  do(mutate(., lower_ci = ifelse(src == "model", NA, lower_ci))) %>%
  do(mutate(., upper_ci = ifelse(src == "model", NA, upper_ci)))

png('../writing/2015/fyp-report/exp4QuestResults.png',
     width = 2000, height=1000,res = 300, pointsize = 12)
dodge <- position_dodge(width=0.9)
g <- (ggplot(plottableQ, aes(x = response, y = prob, fill = src))
      + geom_bar(position = dodge, stat = "identity") 
      + geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), 
                      position = dodge, width = .25)
      + facet_grid(type ~ goal)
      + ggtitle("Pragmatic Questioner"))
g
dev.off()

```

Answerer predictions + empirical data

```{r}
# first, average over domains
collapsed_a <- d %>% 
    mutate(response = ordered(answerNodes,levels=c("A1","A2","A3","A4"))) %>%
    mutate(utterance = ordered(questionNodes,levels = c("Q1","Q2","Q3","Q4"))) %>%
    group_by(type, utterance) %>%
    do(getProbsAndCIs(data = ., QorA = 'a', R = 1000)) %>%
    mutate(empirical = empProb) %>%
    select(utterance, type, response, count, lower_ci, upper_ci,
           groupSize, empirical)

pragModelPreds = ansFits %>% 
  group_by(utterance, type, modelLevel, response) %>% 
  summarize(model = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")

plottableA = collapsed_a %>% 
  inner_join(pragModelPreds, by = c('utterance','type','response')) %>%
  select(utterance, type, response, empirical, model, lower_ci, upper_ci) %>%
  gather(src, prob, empirical, model) %>% 
  do(mutate(., lower_ci = ifelse(src == "model", NA, lower_ci))) %>%
  do(mutate(., upper_ci = ifelse(src == "model", NA, upper_ci)))

png('../writing/2015/fyp-report/exp4AnsResults.png',
     width = 2000, height=1000,res = 300, pointsize = 12)

dodge <- position_dodge(width=0.9)
g <- (ggplot(plottableA, aes(x = response, y = prob, fill = src))
      + geom_bar(position = dodge, stat = "identity") 
      + geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), 
                      position = dodge, width = .25)
      + facet_grid(type ~ utterance)
      + ggtitle("Pragmatic Answerer"))
g
dev.off()
```

zoom in on overlapping condition

```{r}
collapsed_q <- d %>% 
    mutate(response = ordered(questionNodes, levels = c("Q1","Q2","Q3","Q4"))) %>%
    mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
    group_by(goal,type) %>%
    filter(type == "overlapping") %>%
    filter(domain != "places") %>%
    do(getProbsAndCIs(data = ., QorA = 'q', R = 1000)) %>%
    mutate(empirical = empProb) %>%
    select(goal, type, response, count, lower_ci, upper_ci, group_size,
           groupSize, empirical)

pragModelPreds = questFits %>% 
  filter(domain != "places") %>%
  group_by(goal, type, modelLevel, response) %>% 
  summarize(pragmaticModel = mean(modelProb)) %>% 
  filter(modelLevel == "pragmatic")

expModelPreds = questFits %>% 
  filter(domain != "places") %>%
  group_by(goal, type, modelLevel, response) %>% 
  summarize(explicitModel = mean(modelProb)) %>% 
  filter(modelLevel == "explicit")

plottableQ = collapsed_q %>% 
  inner_join(pragModelPreds, by = c('goal', 'type', 'response')) %>%
  inner_join(expModelPreds, by = c('goal', 'type', 'response')) %>%
  select(goal, type, response, lower_ci, upper_ci, groupSize,
         empirical, pragmaticModel, explicitModel) %>%
  gather(src, prob, empirical, pragmaticModel, explicitModel) %>%
  filter(goal == "G2") %>%
  do(mutate(., lower_ci = ifelse(src %in% c("pragmaticModel", "explicitModel"), 
                                 NA, lower_ci))) %>%
  do(mutate(., upper_ci = ifelse(src %in% c("pragmaticModel", "explicitModel"),
                                 NA, upper_ci)))

png(filename = "../writing/2015/fyp-report/OverlappingModelComparison.png",
     width = 2000, height=1000,res = 300, pointsize = 16)

(ggplot(plottableQ, aes(x = response, y = prob, fill = src))
 + geom_bar(position = dodge, stat= 'identity')
 + geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), 
                 position = dodge, width =.25)
 + geom_hline(yintercept = 0.48)
 + ggtitle("Overlapping model comparison")
 + theme_bw()
 + scale_fill_manual(values = c("#B2DF8A", "#A6CEE3", "#62A0CA")))
dev.off()
```

Actually do statistical test to show that they're different...

```{r}
empiricalProbs = subset(plottableQ, 
                        plottableQ$src == "empirical" &
                        plottableQ$type == "overlapping" & 
                        plottableQ$goal == "G2")
empiricalProbs
testStat = $prob
```