---
title: "MultiExp2_analysis"
output: html_document
---

Set wd

```{r}
library(knitr)
opts_knit$set(root.dir = '/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/')
setwd("/Users/rxdh/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/")
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(MultinomialCI)
```

Experiment data analysis
--------------------------

Load data, pull in game ids from the mturk info dump

```{r}
ps = read.csv("./data/MultiExp2_compiled-subject_information.csv",
              sep = ',', header = TRUE)
mturk = read.csv("./data/MultiExp2_compiled-mturk.csv",
                 sep=',', header= TRUE) %>%
  mutate(gameID = Answer.id) %>%
  select(workerid, gameID)
d = read.csv("./data/MultiExp2_compiled-trials.csv",  
             sep = ',', header = TRUE) %>%
  right_join(mturk, by = 'workerid')
```

Filter out participants who didn't complete all 12 trials or didn't list english as their native language, then get rid of duplicate games

```{r}
english_ps <- (ps %>% 
               filter(nativeEnglish == "yes"))$workerid
cat("We removed", length(ps$workerid) - length(english_ps), "due to language")
trialsCompleted = d %>% count(workerid) %>% mutate(numCompleted = n)
d = right_join(d, trialsCompleted, by = "workerid") %>%
    filter(workerid %in% english_ps) %>% 
    filter(numCompleted == 12) %>% 
    filter(answer != "NA") %>%
    distinct(domain, goal, question, guess, answer, type, gameID)    
```

In order to compare different items in a convenient way, we're going to map the questions and answers to the corresponding node positions in the hierarchy. So, in 'branching' trials, for example, 'dalmatian', 'mansion', 'carrot', and 'couch' would all be treated the same.


```{r}

source("~/Box Sync/stanford/research/goodman/q&a/MultiExperiment2/analysis/analysisHelpers.R")
mapWordsToNodes <- function(d) {
  answerNodes = c()
  questionNodes = c()
  goalNodes = c()
  for(i in 1:length(d$workerid)) {
    answerNodes <- append(answerNodes, mapAnswer(d[i,]))
    questionNodes <- append(questionNodes, mapQuestion(d[i,]))
    goalNodes <- append(goalNodes, mapGoal(d[i,]))
  }
  d$goalNodes = goalNodes
  d$answerNodes = answerNodes
  d$questionNodes = questionNodes
  return(d)
}

d <- mapWordsToNodes(d)
```

Tidy up questioner data...

```{r}
d_q = d %>% 
      mutate(response = ordered(questionNodes, levels = c("Q1", "Q2", "Q3", "Q4"))) %>%
      mutate(goal = ordered(goalNodes, levels = c("G1", "G2", "G3", "G4"))) %>%
      group_by(domain, type, goal) %>%
      mutate(group_size = n()) %>%
      group_by(domain, type, goal, response) %>% # collapse over participants
      summarize(count = n(), empProb = n() / mean(group_size))%>%
      select(goal, type, response, count, empProb)

# Hadley Wickham hasn't fixed summarize to take drop = F yet, so we have to re-insert rows with p = 0
d_q = d_q %>%
      right_join(expand.grid(domain = levels(d_q$domain),
                             type = levels(d_q$type),
                             response = levels(d_q$response),
                             goal = levels(d_q$goal))) %>%
      do(mutate(., count = ifelse(is.na(count), 0, count))) %>%
      do(mutate(., empProb = ifelse(is.na(empProb), 0, empProb)))

# get confidence intervals in the stupidest way possible...
qud_levels = unique(d_q$goal)
cis = rbind(multinomialCI(subset(d_q, goal == qud_levels[1])$count, .05),
            multinomialCI(subset(d_q, goal == qud_levels[2])$count, .05),
            multinomialCI(subset(d_q, goal == qud_levels[3])$count, .05),
            multinomialCI(subset(d_q, goal == qud_levels[4])$count, .05))
colnames(cis) <- c("lower_ci", "upper_ci")
d_q = cbind(d_q, cis)
```

Tidy up answerer data...

```{r}
d_a = d %>% 
      mutate(response = ordered(answerNodes,
                                levels=c("A1","A2","A3","A4"))) %>%
      mutate(utterance = ordered(questionNodes,
                                 levels = c("Q1", "Q2", "Q3", "Q4"))) %>%
      group_by(domain, type, utterance) %>%
      mutate(group_size = n()) %>%
      group_by(domain, type, utterance, response) %>% # collapse over participants
      summarize(count = n(), empProb = n() / mean(group_size))%>%
      select(domain, type, utterance, response, empProb, count)

# Hadley Wickham hasn't fixed summarize to take drop = F yet, so we have to re-insert rows with p = 0
d_a = d_a %>% 
      right_join(expand.grid(domain = levels(d_a$domain),
                             type = levels(d_a$type), 
                             response  = levels(d_a$response),
                             utterance = levels(d_a$utterance))) %>%
      do(mutate(., count = ifelse(is.na(count), 0, count))) %>%
      do(mutate(., empProb = ifelse(is.na(empProb), 0, empProb))) 

# And compute confidence intervals
utterance_levels = unique(d_a$utterance)
cis = rbind(multinomialCI(subset(d_a, utterance == utterance_levels[1])$count, .05),
            multinomialCI(subset(d_a, utterance == utterance_levels[2])$count, .05),
            multinomialCI(subset(d_a, utterance == utterance_levels[3])$count, .05),
            multinomialCI(subset(d_a, utterance == utterance_levels[4])$count, .05))
colnames(cis) <- c("lower_ci", "upper_ci")
d_a = cbind(d_a, cis)
```

Some chi-squared tests
----------------------

For each distribution, we run a chi-squared test, as planned

```{r}
overall_distribution = (d %>% group_by(goal, question) %>% tally)$n
test_dal_q = chisq.test(subset(d_q, goal == "dalmatian")$count) #p = overall_distribution/sum(overall_distribution))
test_dal_q
test_poodle_q = chisq.test(subset(d_q, goal == "poodle")$count)
test_poodle_q
test_cat_q = chisq.test(subset(d_q, goal == "siamese cat")$count)
test_cat_q
test_fish_q = chisq.test(subset(d_q, goal == "whale")$count)
test_fish_q

test_dal_a = chisq.test(subset(d_a, utterance == "dalmatian")$count) 
test_dal_a
test_dog_a = chisq.test(subset(d_a, utterance == "dog")$count)
test_dog_a
test_mammal_a = chisq.test(subset(d_a, utterance == "pet")$count)
test_mammal_a
test_animal_a = chisq.test(subset(d_a, utterance == "animal")$count)
test_animal_a
```

Fitting rationality parameters
------------------------------

First, we define this function that takes a data frame and computes which parameter values optimize the correlation between model and data.

```{r}
# Expect two columns that end with _prob (i.e. emp_prob and model_prob)
optimalFit <- function(data, equal = FALSE) {
  prob_correlation <- data %>%
#     group_by(modelLevel, rationality, goal, response, modelProb) %>%
#     summarise(empProb = mean(empProb)) %>%
    group_by(modelLevel, rationality) %>%
    filter(rationality > 1) %>%
    summarise(correlation = cor(modelProb, empProb, method = 'pearson')) %>%
    mutate(m = max(correlation)) %>%
    ungroup() %>%
    filter(m == correlation) %>%
    mutate(maximizingR = rationality) %>%
    group_by(modelLevel, correlation) %>%
    summarize(maximizingR = min(maximizingR)) %>%
    select(modelLevel, maximizingR, correlation)
  print(prob_correlation)
  # add literal back in w/ correlation = NA
  
  #for(domain in levels(data$domain)) {
#     for(type in levels(data$type)) {
#       prob_correlation <- rbind(prob_correlation, c(domain, type, 'literal', 1.0, NA))
#     }
  #}
  return(data %>% join(prob_correlation) %>% 
         filter(rationality == maximizingR))
}
```

Now we import the literal answer fits and tidy them up to eventually be joined
```{r}
ansFits = optimalFit(inner_join(d_a, read.csv("analysis/predictions/answererPredictions.csv", sep = ','))) %>%
  select(domain, modelLevel, type, utterance, response, empProb, rationality, modelProb)
```

Now that each of these data sets is in a nice format, we can join them all together and plot their fits:

```{r}
# join them all together
all_ans = d_a %>% 
  inner_join(ansFits, by = c('domain', 'type', 'utterance', 'response', 'empProb')) %>%
#  group_by(type, modelLevel, utterance, response, modelProb, rationality) %>%
#  summarise(meanEmpProb = mean(emp_prob)) %>%
#  ungroup() %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, utterance, response, empProb, modelLevel, modelProb) 
  #distinct(utterance, response, model_level, model_prob)

# Since they won't let us annotate nicely...
  
answer_plots = (ggplot(all_ans, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_text(angle=90, vjust=1))
  + xlab("Model predicted probability")
  + ylim(0,1)
  + ylab("")
  + geom_point(aes(colour = type))
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + scale_x_continuous(lim = c(0,1), breaks=c(0,.5,1))
  + ggtitle("Answerers")
  + geom_smooth(method = "lm")
  + facet_grid(domain ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = levels(all_ans$modelLevel),
                              domain = levels(all_ans$domain),
                              lab = (all_ans %>% group_by(domain, modelLevel) %>% 
                                    summarise(correlation = paste("r =",
                                                                  round(cor(empProb, modelProb, method = 'pearson'), 2))))$correlation)))
answer_plots               
```

Now we import the literal questioner fits and tidy them up to eventually be joined

```{r}
questFits = optimalFit(inner_join(d_q, read.csv("analysis/predictions/questionerPredictions.csv", sep = ','))) %>%
  select(type, domain, modelLevel, goal, response, empProb, modelProb)
```

Now make the questioner plot

```{r}
# join them all together
all_qs = d_q %>% 
  inner_join(questFits, by = c('type', 'domain', 'goal', 'response', 'empProb')) %>%
#  group_by(type, modelLevel, goal, response, modelProb, rationality) %>%
#  summarise(meanEmpProb = mean(emp_prob)) %>%
#  ungroup() %>%
  mutate(modelLevel = ordered(modelLevel, 
                               levels = c("literal", "explicit", "pragmatic"))) %>%
  select(type, domain, goal, response, empProb, modelLevel, modelProb) 
  #distinct(type, domain, goal, response, modelLevel, modelProb)

# Since they won't let us annotate nicely...
  
labelDataFrame = all_qs %>% 
   group_by(domain, modelLevel) %>% 
   summarise(correlation = paste("r =", round(cor(empProb, 
                                                  modelProb, method = 'pearson'), 2)))

#jpeg(filename="../writing/2015/cogsci/questionerFits.jpeg")
question_plots = (ggplot(all_qs, aes(x = modelProb, y = empProb))
  + theme(text = element_text(size = 20),
          axis.text.x = element_blank(), axis.ticks = element_blank(),
          plot.margin=unit(c(1,1,-1,1), "cm"))
  + ylim(0,1)
  + xlab("")
  + ylab("")
  + geom_point(aes(colour = type))
  + ggtitle("Questioners")
  + geom_abline(intercept = 0, slope = 1, linetype = "dotted")
  + geom_smooth(method = "lm")
  + facet_grid(domain ~ modelLevel)
  + geom_text(aes(x,y,label=lab),
              data=data.frame(x = .75, y = .25,
                              modelLevel = labelDataFrame$modelLevel,
                              domain = labelDataFrame$domain,
                              lab = labelDataFrame$correlation)))
question_plots               
#dev.off()
```

Put these next to each other
```{r}
pdf("model_fits_grid.pdf")
grid.newpage()
grid.draw(rbind(ggplotGrob(question_plots), ggplotGrob(answer_plots), size="last"))
grid.draw(textGrob("Empirical probability", rot = 90, vjust = 1, 
                   x = unit(0.01, "npc"), y = unit(0.5, "npc"),
                   gp=gpar(fontsize=20)))
dev.off()
```

Model + data bar plots
----------------------

Plot for pragmatic questioner. These bar graphs will help show what our model is getting right and what it's getting wrong.

```{r}
fitted_data = (prag_quest_fits %>% rename(model_prob =pragmatic_prob))
plot_title = "Pragmatic_Questioner"
# fitted_data = (exp_quest_fits %>% rename(model_prob = explicit_prob)) 
# plot_title = "Explicit_Questioner"

new_labels = as.factor(sapply(X = fitted_data$goal, FUN = function(v) {return(paste("goal:", v))}))
fitted_data$facet_label = ordered(new_labels,
                             levels = c("goal: dalmatian", "goal: poodle", 
                                        "goal: siamese cat", "goal: whale"))
q_comparison <- fitted_data %>% 
  select(goal, response, emp_prob, upper_ci, lower_ci, model_prob, facet_label) %>%
  rename(empirical = emp_prob, model = model_prob) %>%
  gather(src, prob, empirical, model) 
print(q_comparison)
# Hack to set confidence intervals to 0 for the model
q_comparison[q_comparison$src == "model",]$upper_ci = q_comparison[q_comparison$src == "model",]$prob
q_comparison[q_comparison$src == "model",]$lower_ci = q_comparison[q_comparison$src == "model",]$prob
name = paste(plot_title, ".pdf", sep = '')
pdf(name)
dodge <- position_dodge(width=0.9)
g4<-(ggplot(q_comparison, aes(x=response, y=prob, fill=src)) 
    #+ scale_y_continuous(limits = c(0,.3))
    + geom_bar(stat='identity', position=dodge)
    + geom_errorbar(aes(ymax = upper_ci, ymin = lower_ci), 
                    position=dodge, width = .25)
    + ylim(-.1, 1.1)
    + scale_fill_grey()
    + theme_bw(base_size = 20)
    + theme(axis.text.x = element_text(angle=90, vjust=1))
    + ggtitle(plot_title)
    + facet_wrap(~facet_label))
g4
dev.off()
```

Now, plot for answerer model:

```{r}
fitted_data = (prag_ans_fits %>% rename(model_prob = pragmatic_prob)) 
plot_title = "Pragmatic_Answerer"
#fitted_data = (exp_ans_fits %>% rename(model_prob = explicit_prob)) 
#plot_title = "Explicit_Answerer"

new_labels = as.factor(sapply(X = fitted_data$utterance, FUN = function(v) {return(paste("utterance:", v))}))
fitted_data$facet_label = ordered(new_labels, 
                              levels = c("utterance: dalmatian", "utterance: dog", 
                                         "utterance: pet","utterance: animal"))
a_comparison <- fitted_data %>% 
  select(utterance, response, emp_prob, upper_ci, lower_ci, model_prob, facet_label) %>%
  rename(empirical = emp_prob, model = model_prob) %>%
  gather(src, prob, empirical, model) 

a_comparison[a_comparison$src == "model",]$upper_ci = a_comparison[a_comparison$src == "model",]$prob
a_comparison[a_comparison$src == "model",]$lower_ci = a_comparison[a_comparison$src == "model",]$prob
name = paste(plot_title, ".pdf", sep = '')
pdf(name)
dodge <- position_dodge(width=0.9)
g4<-(ggplot(a_comparison, aes(x=response, y = prob, fill=src)) 
     + geom_bar(stat='identity', position=position_dodge())
     + geom_errorbar(aes(ymax = upper_ci, ymin = lower_ci), 
                    position=dodge, width = .25)
     + scale_fill_grey()
     + theme_bw(base_size = 20)
     + ggtitle(plot_title)
     + theme(axis.text.x = element_text(angle=90, vjust=1))
     + facet_wrap(~facet_label))
g4
dev.off()
```

quick plots

```{r}
qplot(x = response, y = emp_prob,
      data = subset(d_a, domain == "places" & type == "overlapping"), 
      facets = type ~ utterance, 
      geom = 'bar', stat = 'identity', position = 'dodge')
```

full answer

```{r}
qplot(x = response, y = emp_prob, fill = domain,
      data = d_a, facets = type ~ utterance, 
      geom = 'bar', stat = 'identity',  position = 'dodge')
```

full question

```{r}
qplot(x = response, y = empProb, fill = domain,
      data = d_q, facets = type ~ goal, 
      geom = 'bar', stat = 'identity',  position = 'dodge')
```